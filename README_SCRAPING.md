# ğŸ•·ï¸ Module de Scraping Pappers.fr

## ğŸ“‹ Vue d'ensemble

Le module de scraping permet d'enrichir vos donnÃ©es **gratuitement** sans clÃ© API, en naviguant sur les pages publiques de Pappers.fr avec des dÃ©lais alÃ©atoires pour Ã©viter la dÃ©tection.

âš ï¸ **Important**: Le scraping est un **fallback gratuit** mais prÃ©sente des limitations. L'API officielle est recommandÃ©e pour un usage professionnel.

---

## ğŸ”„ Modes d'enrichissement

### 1. Mode API (RecommandÃ©)
- âœ… Rapide (0.5s par entreprise)
- âœ… Fiable et stable
- âœ… DonnÃ©es structurÃ©es garanties
- âŒ Payant (20-100â‚¬/mois selon volume)

### 2. Mode Scraping (Gratuit)
- âœ… **Gratuit**
- âœ… Pas de limite d'utilisation
- âš ï¸ Plus lent (2-5s par entreprise)
- âš ï¸ Peut Ãªtre bloquÃ©
- âš ï¸ Fragile (HTML peut changer)

### 3. Mode Hybride (Optimal)
- âœ… API en prioritÃ©
- âœ… Scraping en fallback si API Ã©choue
- âœ… Meilleure rÃ©silience

---

## âš™ï¸ Configuration

### Fichier `.env`

```env
# ============================================
# MODE API (RecommandÃ©)
# ============================================
PAPPERS_API_KEY=votre_cle_api_ici
PAPPERS_DELAY_SECONDS=0.5

# ============================================
# MODE SCRAPING (Fallback gratuit)
# ============================================
SCRAPING_ENABLED=true

# DÃ©lais alÃ©atoires (IMPORTANT pour Ã©viter le blocage)
SCRAPING_MIN_DELAY=2.0
SCRAPING_MAX_DELAY=5.0
```

### ScÃ©narios de configuration

#### 1. API uniquement
```env
PAPPERS_API_KEY=ma_cle_secrete
SCRAPING_ENABLED=false
```

#### 2. Scraping uniquement (gratuit)
```env
SCRAPING_ENABLED=true
SCRAPING_MIN_DELAY=3.0
SCRAPING_MAX_DELAY=7.0
```

#### 3. Hybride (recommandÃ©)
```env
PAPPERS_API_KEY=ma_cle_secrete
SCRAPING_ENABLED=true  # Fallback si API Ã©choue
```

---

## ğŸš€ Utilisation

### Test rapide

```bash
python test_scraping.py
```

### Dans votre code

```python
from enrichment_pappers import get_company_data_unified

# Mode automatique (API â†’ Scraping)
data = get_company_data_unified('449162163')

# Forcer le scraping
data = get_company_data_unified('449162163', prefer_api=False)
```

### Interface Streamlit

```bash
streamlit run app_pappers.py
```

L'interface dÃ©tecte automatiquement le mode disponible :
- **ClÃ© API configurÃ©e** â†’ Mode API avec fallback scraping
- **Pas de clÃ© API** â†’ Mode scraping uniquement

---

## ğŸ›¡ï¸ MÃ©canismes anti-dÃ©tection

### 1. DÃ©lais alÃ©atoires

```python
# Entre chaque requÃªte: 2-5 secondes (alÃ©atoire)
delay = random.uniform(2.0, 5.0)
time.sleep(delay)
```

**Pourquoi c'est important:**
- âœ… Simule un comportement humain
- âœ… Ã‰vite les patterns suspects
- âœ… RÃ©duit le risque de blocage

### 2. Rotation des User-Agents

```python
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64) Firefox/121.0',
    # ... 5 user agents diffÃ©rents
]

# SÃ©lection alÃ©atoire Ã  chaque requÃªte
headers = {'User-Agent': random.choice(USER_AGENTS)}
```

### 3. Headers HTTP rÃ©alistes

```python
headers = {
    'User-Agent': random_ua,
    'Accept': 'text/html,application/xhtml+xml,...',
    'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.7',
    'Accept-Encoding': 'gzip, deflate, br',
    'DNT': '1',
    'Connection': 'keep-alive',
    'Sec-Fetch-Dest': 'document',
    'Sec-Fetch-Mode': 'navigate',
    # ... headers complets
}
```

### 4. Retry automatique avec backoff

```python
# 3 tentatives avec dÃ©lais croissants
for attempt in range(3):
    try:
        response = requests.get(url)
        if response.status_code == 429:
            wait = random.uniform(2, 5) * (2 ** attempt)
            time.sleep(wait)
    except:
        time.sleep(random.uniform(2, 5))
```

---

## ğŸ“Š Performance

### Temps de traitement

| Mode | Temps/entreprise | 100 entreprises | 1000 entreprises |
|------|------------------|-----------------|------------------|
| **API** | 0.5s | ~50s | ~8min |
| **Scraping** | 2-5s (alÃ©atoire) | ~6min | ~1h |
| **Hybride** | 0.5-5s (variable) | ~2-6min | ~10-60min |

### Taux de succÃ¨s typique

- **API**: 95-98% (instabilitÃ© rÃ©seau)
- **Scraping**: 70-85% (blocages, timeouts)
- **Hybride**: 90-95% (combine les deux)

---

## âš ï¸ Limitations du scraping

### 1. InstabilitÃ©
- La structure HTML de Pappers peut changer sans prÃ©avis
- Les sÃ©lecteurs CSS peuvent devenir obsolÃ¨tes
- NÃ©cessite maintenance rÃ©guliÃ¨re

### 2. Blocage possible
Pappers peut bloquer si :
- Trop de requÃªtes en peu de temps
- Pattern suspect dÃ©tectÃ©
- IP mise sur liste noire

**Solutions:**
```env
# Augmenter les dÃ©lais
SCRAPING_MIN_DELAY=5.0
SCRAPING_MAX_DELAY=10.0
```

### 3. DonnÃ©es incomplÃ¨tes
Le scraping peut manquer certaines donnÃ©es si :
- Structure HTML diffÃ©rente de prÃ©vue
- DonnÃ©es dynamiques chargÃ©es en JavaScript
- Format inattendu

### 4. Aspects lÃ©gaux
âš ï¸ **VÃ©rifiez les CGU de Pappers.fr** avant usage intensif
- Le scraping peut violer les conditions d'utilisation
- Usage commercial peut nÃ©cessiter autorisation
- PrivilÃ©giez l'API officielle pour Ã©viter tout problÃ¨me

---

## ğŸ› DÃ©pannage

### âŒ "Aucune donnÃ©e rÃ©cupÃ©rÃ©e"

**Causes possibles:**
1. Structure HTML de Pappers a changÃ©
2. Scraping bloquÃ© par Pappers
3. DÃ©lais trop courts

**Solutions:**
```bash
# Tester manuellement
python test_scraping.py

# Augmenter dÃ©lais
SCRAPING_MIN_DELAY=7.0
SCRAPING_MAX_DELAY=12.0

# VÃ©rifier les logs
python enrichment_pappers.py
```

### âŒ "Timeout errors"

**Solution:**
```python
# Augmenter timeout dans enrichment_pappers.py
response = requests.get(url, timeout=60)  # Au lieu de 30
```

### âŒ "Rate limit 429"

**Solution:**
```env
# DÃ©lais beaucoup plus longs
SCRAPING_MIN_DELAY=10.0
SCRAPING_MAX_DELAY=20.0
```

### âŒ "DonnÃ©es financiÃ¨res mal extraites"

Le parsing HTML peut Ã©chouer si la structure change.

**Solution:**
1. VÃ©rifier manuellement sur pappers.fr
2. Utiliser l'API officielle (plus fiable)
3. Signaler le problÃ¨me pour mise Ã  jour du code

---

## ğŸ’¡ Bonnes pratiques

### 1. Commencer petit
```python
# Tester sur 5-10 entreprises d'abord
df_test = df.head(10)
enriched = enrich_with_pappers(df_test)
```

### 2. Sauvegardes frÃ©quentes
```python
# Checkpoint tous les 50 SIREN
for i in range(0, len(df), 50):
    batch = enrich_with_pappers(df[i:i+50])
    batch.to_excel(f'backup_{i}.xlsx', index=False)
```

### 3. Heures creuses
- Scraper la nuit (moins de surveillance)
- Week-ends (trafic plus faible)
- Ã‰viter 9h-18h en semaine

### 4. Rotation d'IP (avancÃ©)
Pour usage intensif, utilisez des proxies :
```python
proxies = {
    'http': 'http://proxy1.com:8080',
    'https': 'https://proxy1.com:8080'
}
response = requests.get(url, proxies=proxies)
```

### 5. Monitoring
```python
# Logger les Ã©checs
import logging
logging.basicConfig(filename='scraping.log', level=logging.INFO)
```

---

## ğŸ¯ Quand utiliser chaque mode

### Utilisez l'API si:
- âœ… Budget disponible (20-100â‚¬/mois)
- âœ… Usage rÃ©gulier/professionnel
- âœ… Besoin de fiabilitÃ©
- âœ… Gros volumes (>100 entreprises/jour)

### Utilisez le scraping si:
- âœ… Test/prototype
- âœ… Usage ponctuel
- âœ… Budget limitÃ©
- âœ… Petits volumes (<50 entreprises/jour)

### Utilisez le mode hybride si:
- âœ… Budget limitÃ© mais besoin de fiabilitÃ©
- âœ… Volumes moyens (50-200/jour)
- âœ… TolÃ©rance aux temps variables

---

## ğŸ“ˆ AmÃ©liorer le taux de succÃ¨s

### 1. DÃ©lais gÃ©nÃ©reux
```env
SCRAPING_MIN_DELAY=5.0  # Plus lent mais plus sÃ»r
SCRAPING_MAX_DELAY=10.0
```

### 2. Filtrer en amont
```python
# Scraper seulement les grandes entreprises
df_filtered = df[df['Effectif'] > 50]
```

### 3. VÃ©rifier la disponibilitÃ©
```python
# VÃ©rifier qu'une page existe avant scraping
response = requests.head(url)
if response.status_code == 200:
    data = scrape_company_data_pappers(siren)
```

---

## ğŸ”’ ConsidÃ©rations lÃ©gales

### âš ï¸ IMPORTANT

Le scraping web soulÃ¨ve des questions lÃ©gales :

1. **CGU de Pappers.fr**: VÃ©rifiez qu'ils autorisent le scraping
2. **Usage commercial**: Peut nÃ©cessiter licence
3. **DonnÃ©es personnelles**: RGPD applicable
4. **PropriÃ©tÃ© intellectuelle**: Contenus potentiellement protÃ©gÃ©s

**Recommandations:**
- ğŸ“§ Contactez Pappers pour autorisation
- ğŸ“œ Lisez attentivement les CGU
- ğŸ’¼ Pour usage professionnel â†’ API officielle
- ğŸ“ Pour recherche/Ã©ducation â†’ OK avec prÃ©cautions

---

## ğŸ“ Support

- **Documentation Pappers**: [pappers.fr/api/documentation](https://www.pappers.fr/api/documentation)
- **CGU Pappers**: [pappers.fr/cgu](https://www.pappers.fr/cgu)
- **Alternative lÃ©gale**: API officielle Pappers

---

## âœ… Checklist avant utilisation

- [ ] DÃ©lais configurÃ©s (min 2s)
- [ ] Mode hybride activÃ© (API + scraping)
- [ ] Test sur petit Ã©chantillon effectuÃ©
- [ ] Sauvegardes automatiques en place
- [ ] CGU Pappers.fr consultÃ©es
- [ ] Monitoring des erreurs actif
- [ ] Plan B (API) prÃ©vu si blocage

---

**ğŸ¯ PrÃªt Ã  scraper ?** Lancez `python test_scraping.py` pour tester !
