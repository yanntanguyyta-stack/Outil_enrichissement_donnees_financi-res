"""
Streamlit app for searching French companies using data.gouv.fr API.

Uses the unified enrichment module (enrichment.py) which combines:
  - DINUM API for company identification
  - SQLite database for financial data (built from RNE/INPI)

IMPORTANT: This app only uses REAL data from the official French government API.
           No demo or fake data is returned.
"""
import streamlit as st
import pandas as pd
from io import BytesIO
from pathlib import Path
import re
import time
import requests
import math
import os

# Import unified enrichment module
try:
    from enrichment import get_finances, db_available, db_age_days
    FINANCES_AVAILABLE = True
except ImportError:
    FINANCES_AVAILABLE = False

st.set_page_config(
    page_title="Recherche Entreprises",
    page_icon="üè¢",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# ‚îÄ‚îÄ Minimalist CSS ‚îÄ‚îÄ
st.markdown("""
<style>
    /* Clean, minimal spacing */
    .block-container { padding-top: 2rem; max-width: 960px; }

    /* Subtle header */
    .app-header {
        padding: 1.2rem 0 0.8rem 0;
        border-bottom: 2px solid #e0e0e0;
        margin-bottom: 1.5rem;
    }
    .app-header h1 { font-size: 1.6rem; font-weight: 600; margin: 0; color: #1a1a2e; }
    .app-header p  { font-size: 0.9rem; color: #666; margin: 0.3rem 0 0 0; }

    /* Clean buttons */
    .stButton > button {
        border-radius: 6px;
        font-weight: 500;
        padding: 0.5rem 1.5rem;
    }

    /* Compact metrics */
    [data-testid="stMetricValue"] { font-size: 1.5rem; font-weight: 600; }

    /* Remove sidebar shadow */
    [data-testid="stSidebar"] { background: #fafafa; }
    
    /* Info boxes styling */
    .stAlert { margin-top: 0.5rem; margin-bottom: 0.5rem; }
</style>
""", unsafe_allow_html=True)

# ‚îÄ‚îÄ Header ‚îÄ‚îÄ
st.markdown("""
<div class="app-header">
    <h1>üè¢ Recherche d'Entreprises</h1>
    <p>Donn√©es officielles ‚Äî API DINUM &amp; RNE / INPI</p>
</div>
""", unsafe_allow_html=True)

# API configuration
USE_API = True
API_BASE_URL = "https://recherche-entreprises.api.gouv.fr"
API_DELAY_SECONDS = float(os.getenv("DINUM_API_DELAY_SECONDS", "0.8"))
API_MAX_RETRIES = 3
DB_AGE_WARNING_DAYS = 90
API_PAGE_SIZE = 25
API_EXTRACTION_LIMIT_MAX = 2000
API_MAX_DELAY_SECONDS = float(os.getenv("DINUM_API_MAX_DELAY_SECONDS", "8"))
API_IMPORT_MAX_COMPANIES = int(os.getenv("DINUM_IMPORT_MAX_COMPANIES", "1500"))

API_SESSION = requests.Session()

CATEGORIE_ENTREPRISE_OPTIONS = ["PME", "ETI", "GE"]
ETAT_ADMIN_OPTIONS = ["A", "C"]
SECTION_ACTIVITE_OPTIONS = list("ABCDEFGHIJKLMNOPQRSTU")
TRANCHE_EFFECTIF_OPTIONS = ["NN", "00", "01", "02", "03", "11", "12", "21", "22", "31", "32", "41", "42", "51", "52", "53"]
DEPARTEMENT_OPTIONS = [f"{i:02d}" for i in range(1, 96)] + ["2A", "2B", "971", "972", "973", "974", "976"]
REGION_OPTIONS = ["01", "02", "03", "04", "06", "11", "24", "27", "28", "32", "44", "52", "53", "75", "76", "84", "93", "94"]
BOOLEAN_FILTER_LABELS = {
    "est_association": "Association",
    "est_entrepreneur_individuel": "Entrepreneur individuel",
    "est_organisme_formation": "Organisme de formation",
    "est_ess": "√âconomie sociale et solidaire",
    "est_qualiopi": "Qualiopi",
    "est_rge": "RGE",
    "est_bio": "Bio",
    "est_service_public": "Service public",
    "est_societe_mission": "Soci√©t√© √† mission",
    "est_collectivite_territoriale": "Collectivit√© territoriale",
}

# DB age warning
if FINANCES_AVAILABLE and db_available():
    age = db_age_days()
    if age is not None and age > DB_AGE_WARNING_DAYS:
        st.warning(f"‚ö†Ô∏è Base financi√®re dat√©e de {age} jours. Lancez `python update_rne_db.py` pour la mettre √† jour.")


def is_siret(value):
    """Check if a string looks like a SIRET number (14 digits)."""
    return bool(re.match(r'^\d{14}$', value.strip()))


def is_siren(value):
    """Check if a string looks like a SIREN number (9 digits)."""
    return bool(re.match(r'^\d{9}$', value.strip()))


def extract_siren_from_siret(siret):
    """Extract the SIREN (first 9 digits) from a SIRET (14 digits)."""
    siret = siret.strip()
    if is_siret(siret):
        return siret[:9]
    return siret


def search_company_api(query):
    """Search for a company by name, SIREN or SIRET using the API.
    
    Includes retry logic with exponential backoff for 429 errors.
    """
    # If SIRET, extract SIREN for the API search
    search_query = query.strip()
    if is_siret(search_query):
        search_query = extract_siren_from_siret(search_query)

    params = {"q": search_query, "per_page": 1}
    data = _request_search_api(params=params, timeout=10, query_for_log=query)
    if data and data.get("results") and len(data["results"]) > 0:
        return data["results"][0]
    return None


def _init_api_runtime_state():
    if "api_next_request_ts" not in st.session_state:
        st.session_state["api_next_request_ts"] = 0.0
    if "api_current_delay" not in st.session_state:
        st.session_state["api_current_delay"] = API_DELAY_SECONDS
    if "api_rate_limit_hits" not in st.session_state:
        st.session_state["api_rate_limit_hits"] = 0
    if "api_retry_attempts" not in st.session_state:
        st.session_state["api_retry_attempts"] = 0


def _get_retry_after_seconds(response):
    retry_after = response.headers.get("Retry-After")
    if not retry_after:
        return None
    try:
        return max(0, int(float(retry_after)))
    except ValueError:
        return None


def _wait_for_rate_limit_slot():
    _init_api_runtime_state()
    next_allowed_ts = float(st.session_state.get("api_next_request_ts", 0.0))
    now = time.time()
    wait_seconds = max(0.0, next_allowed_ts - now)
    if wait_seconds > 0:
        time.sleep(wait_seconds)


def _on_api_success():
    _init_api_runtime_state()
    current_delay = float(st.session_state.get("api_current_delay", API_DELAY_SECONDS))
    relaxed_delay = max(API_DELAY_SECONDS, current_delay * 0.95)
    st.session_state["api_current_delay"] = relaxed_delay
    st.session_state["api_next_request_ts"] = time.time() + relaxed_delay


def _on_api_rate_limited(response, attempt):
    _init_api_runtime_state()
    st.session_state["api_rate_limit_hits"] = int(st.session_state.get("api_rate_limit_hits", 0)) + 1
    st.session_state["api_retry_attempts"] = int(st.session_state.get("api_retry_attempts", 0)) + 1

    retry_after = _get_retry_after_seconds(response)
    current_delay = float(st.session_state.get("api_current_delay", API_DELAY_SECONDS))
    exponential_delay = current_delay * (1.7 ** attempt)
    target_delay = retry_after if retry_after is not None else exponential_delay
    new_delay = min(API_MAX_DELAY_SECONDS, max(current_delay, target_delay))

    st.session_state["api_current_delay"] = new_delay
    st.session_state["api_next_request_ts"] = time.time() + new_delay


def _request_search_api(params, timeout=10, query_for_log=""):
    url = f"{API_BASE_URL}/search"
    _init_api_runtime_state()

    for attempt in range(API_MAX_RETRIES):
        try:
            _wait_for_rate_limit_slot()
            response = API_SESSION.get(url, params=params, timeout=timeout)

            if response.status_code == 429:
                _on_api_rate_limited(response, attempt)
                if attempt < API_MAX_RETRIES - 1:
                    continue
                return None

            if response.status_code == 400:
                try:
                    message = response.json().get("erreur", "Param√®tres invalides")
                except Exception:
                    message = "Param√®tres invalides"
                if query_for_log:
                    st.error(f"‚ùå Requ√™te invalide DINUM ({query_for_log}) : {message}")
                else:
                    st.error(f"‚ùå Requ√™te invalide DINUM : {message}")
                return None

            response.raise_for_status()
            _on_api_success()
            return response.json()

        except requests.exceptions.RequestException as e:
            st.session_state["api_retry_attempts"] = int(st.session_state.get("api_retry_attempts", 0)) + 1
            if attempt == API_MAX_RETRIES - 1:
                if query_for_log:
                    st.warning(f"‚ö†Ô∏è API non accessible pour '{query_for_log}': {str(e)}")
                else:
                    st.warning(f"‚ö†Ô∏è API non accessible : {str(e)}")
                return None

    return None


def _parse_multi_values(raw_text):
    values = re.split(r"[,;\n\s]+", (raw_text or "").strip())
    clean_values = [v.strip() for v in values if v and v.strip()]
    dedup = list(dict.fromkeys(clean_values))
    return dedup


def _normalize_naf_code(value):
    if value is None:
        return None

    raw = str(value).strip().upper()
    raw = re.sub(r"[^0-9A-Z.]", "", raw)

    if re.match(r"^\d{2}\.\d{2}[A-Z]$", raw):
        return raw

    compact = raw.replace(".", "")
    if re.match(r"^\d{4}[A-Z]$", compact):
        return f"{compact[:2]}.{compact[2:4]}{compact[4]}"

    return None


def _normalize_naf_codes(values):
    normalized = []
    rejected = []

    for value in values or []:
        naf = _normalize_naf_code(value)
        if naf:
            normalized.append(naf)
        else:
            rejected.append(str(value))

    normalized = list(dict.fromkeys(normalized))
    return normalized, rejected


@st.cache_data(ttl=24 * 3600)
def _load_allowed_naf_codes():
    # 1) Tentative via OpenAPI (quand l'enum est pr√©sente)
    try:
        resp = requests.get(f"{API_BASE_URL}/openapi.json", timeout=20)
        resp.raise_for_status()
        spec = resp.json()
        params = (
            spec.get("paths", {})
            .get("/search", {})
            .get("get", {})
            .get("parameters", [])
        )
        for param in params:
            if param.get("name") == "activite_principale":
                allowed = param.get("schema", {}).get("enum", [])
                if allowed:
                    return set(allowed)
    except Exception:
        pass

    # 2) Fallback via message d'erreur API (contient la liste exhaustive)
    try:
        probe = requests.get(
            f"{API_BASE_URL}/search",
            params={"q": "transport", "per_page": 1, "activite_principale": "99.99X"},
            timeout=20,
        )
        if probe.status_code == 400:
            payload = probe.json()
            erreur = str(payload.get("erreur", ""))
            values = re.findall(r"'([0-9]{2}\.[0-9]{2}[A-Z])'", erreur)
            if values:
                return set(values)
    except Exception:
        pass

    return set()


def _resolve_naf_codes_for_api(normalized_codes):
    allowed = _load_allowed_naf_codes()
    if not allowed:
        return normalized_codes, [], {}

    resolved = []
    rejected = []
    remapped = {}

    for code in normalized_codes or []:
        if code in allowed:
            resolved.append(code)
            continue

        prefix = code[:5] if len(code) >= 5 else code
        family = sorted([c for c in allowed if c.startswith(prefix)])
        if family:
            remapped[code] = family
            resolved.extend(family)
        else:
            rejected.append(code)

    resolved = list(dict.fromkeys(resolved))
    return resolved, rejected, remapped


def _build_search_params(query, filters, page=1, per_page=1):
    params = [("q", query.strip()), ("page", page), ("per_page", per_page)]

    multi_fields = [
        "activite_principale",
        "section_activite_principale",
        "tranche_effectif_salarie",
        "categorie_entreprise",
        "etat_administratif",
        "departement",
        "region",
        "nature_juridique",
        "code_postal",
        "code_commune",
    ]
    for field in multi_fields:
        for value in filters.get(field, []):
            params.append((field, str(value).strip()))

    for boolean_field in filters.get("boolean_flags", []):
        params.append((boolean_field, "true"))

    for numeric_field in ["ca_min", "ca_max", "resultat_net_min", "resultat_net_max"]:
        value = filters.get(numeric_field)
        if value is not None and value != "":
            params.append((numeric_field, int(value)))

    return params


def _resolve_effective_query(raw_query, filters):
    query = (raw_query or "").strip()
    if len(query) >= 2:
        return query, "manual"

    activites = filters.get("activite_principale", []) or []
    if activites:
        return "entreprise", "naf"

    sections = filters.get("section_activite_principale", []) or []
    if sections:
        return "entreprise", "section_naf"

    natures = filters.get("nature_juridique", []) or []
    if natures:
        return "entreprise", "nature_juridique"

    departements = filters.get("departement", []) or []
    if departements:
        return "entreprise", "departement"

    regions = filters.get("region", []) or []
    if regions:
        return "entreprise", "region"

    return "entreprise", "fallback"


def _search_api_with_params(params):
    return _request_search_api(params=params, timeout=15)


def count_companies_api(query, filters):
    params = _build_search_params(query, filters, page=1, per_page=1)
    data = _search_api_with_params(params)
    if not data:
        return None

    return {
        "total_results": int(data.get("total_results", 0)),
        "total_pages": int(data.get("total_pages", 0)),
        "per_page": int(data.get("per_page", 1) or 1),
    }


def fetch_companies_api(query, filters, extraction_limit):
    first_params = _build_search_params(query, filters, page=1, per_page=API_PAGE_SIZE)
    first_page = _search_api_with_params(first_params)
    if not first_page:
        return [], 0

    total_results = int(first_page.get("total_results", 0))
    all_results = list(first_page.get("results", []))
    max_to_fetch = min(total_results, extraction_limit)

    if max_to_fetch <= len(all_results):
        return all_results[:max_to_fetch], total_results

    total_pages = int(first_page.get("total_pages", 1) or 1)
    pages_to_fetch = min(total_pages, math.ceil(max_to_fetch / API_PAGE_SIZE))
    progress = st.progress(0.0)

    for page in range(2, pages_to_fetch + 1):
        page_params = _build_search_params(query, filters, page=page, per_page=API_PAGE_SIZE)
        page_data = _search_api_with_params(page_params)
        if not page_data:
            break

        page_results = page_data.get("results", [])
        if not page_results:
            break

        all_results.extend(page_results)
        progress.progress(min(page / pages_to_fetch, 1.0))

        if len(all_results) >= max_to_fetch:
            break

    progress.empty()
    return all_results[:max_to_fetch], total_results


def companies_to_results(companies, use_rne=False):
    results = []
    total = len(companies)
    progress = st.progress(0.0) if total > 1 else None

    for idx, company_data in enumerate(companies, 1):
        siege = company_data.get("siege") or {}
        original_siret = siege.get("siret", company_data.get("siret"))

        rne_data = None
        if use_rne and FINANCES_AVAILABLE:
            siren = company_data.get("siren")
            if siren:
                rne_data = get_finances(siren)

        info = extract_financial_info(company_data, original_siret, rne_data)
        results.append(info)

        if progress:
            progress.progress(idx / total)

    if progress:
        progress.empty()

    return results


def extract_financial_info(company_data, original_siret=None, rne_data=None):
    """Extract comprehensive information from company data.

    Args:
        company_data: Donn√©es de l'API DINUM
        original_siret: SIRET original de la requ√™te
        rne_data: Donn√©es financi√®res SQLite/RNE (optionnel)
    """
    
    # Base structures
    finances = company_data.get("finances") or {}
    siege = company_data.get("siege") or {}
    complements = company_data.get("complements") or {}
    dirigeants = company_data.get("dirigeants") or []
    
    siren = company_data.get("siren", "N/A")
    siret = original_siret or siege.get("siret", company_data.get("siret", "N/A"))
    
    # SIREN verification status
    siren_verifie = "‚úÖ V√©rifi√©" if siren and siren != "N/A" else "‚ùå Non trouv√©"
    
    # Financial data - utiliser RNE en priorit√© si disponible, sinon DINUM
    ca = "N/A"
    resultat_net = "N/A"
    annee_finance = "N/A"
    finances_publiees = "Non"
    nb_exercices_rne = 0
    source_finances = "N/A"
    
    # Historique financier par ann√©e (depuis 2019)
    historical_data = {}
    
    # Priorit√© aux donn√©es RNE si disponibles
    if rne_data and rne_data.get("success"):
        bilans = rne_data.get("bilans", [])
        if bilans:
            # Prendre le bilan le plus r√©cent
            latest_bilan = bilans[0]
            annee_finance = latest_bilan.get("date_cloture", "N/A")
            ca = latest_bilan.get("chiffre_affaires", "N/A")
            resultat_net = latest_bilan.get("resultat_net", "N/A")
            nb_exercices_rne = len(bilans)
            finances_publiees = "Oui"
            source_finances = f"RNE ({nb_exercices_rne} exercice(s))"
            
            # Construire l'historique par ann√©e (depuis 2019)
            for bilan in bilans:
                date_cloture = bilan.get("date_cloture", "")
                if date_cloture:
                    annee = date_cloture[:4] if len(date_cloture) >= 4 else ""
                    if annee and annee.isdigit() and int(annee) >= 2019:
                        historical_data[annee] = {
                            "ca": bilan.get("chiffre_affaires"),
                            "resultat_net": bilan.get("resultat_net"),
                            "resultat_exploitation": bilan.get("resultat_exploitation"),
                            "total_actif": bilan.get("total_actif"),
                            "capitaux_propres": bilan.get("capitaux_propres"),
                            "effectif": bilan.get("effectif"),
                            "date_cloture": date_cloture,
                        }
    # Sinon, utiliser les donn√©es DINUM
    elif finances:
        # L'API retourne un dict avec l'ann√©e comme cl√©: {"2024": {"ca": ..., "resultat_net": ...}}
        latest_year = max(finances.keys()) if finances else None
        if latest_year:
            annee_finance = latest_year
            year_data = finances[latest_year]
            ca = year_data.get("ca", "N/A")
            resultat_net = year_data.get("resultat_net", "N/A")
            finances_publiees = "Oui"
            source_finances = "API DINUM"
    
    # Dirigeants - formater la liste
    dirigeants_str = "N/A"
    if dirigeants:
        dir_list = []
        for d in dirigeants[:5]:  # Limiter √† 5 premiers
            if d.get("type_dirigeant") == "personne physique":
                nom = f"{d.get('prenoms', '')} {d.get('nom', '')}".strip()
                qualite = d.get('qualite', '')
                dir_list.append(f"{nom} ({qualite})")
            else:
                denom = d.get('denomination', '')
                qualite = d.get('qualite', '')
                dir_list.append(f"{denom} ({qualite})")
        dirigeants_str = " | ".join(dir_list)
        if len(dirigeants) > 5:
            dirigeants_str += f" | ... (+{len(dirigeants)-5})"
    
    # Coordonn√©es g√©ographiques
    latitude = siege.get("latitude", "N/A")
    longitude = siege.get("longitude", "N/A")
    coords = f"{latitude}, {longitude}" if latitude != "N/A" and longitude != "N/A" else "N/A"
    
    # Certifications et labels
    certifications = []
    if complements.get("est_qualiopi"):
        certifications.append("Qualiopi")
    if complements.get("est_rge"):
        certifications.append("RGE")
    if complements.get("est_bio"):
        certifications.append("Bio")
    if complements.get("est_ess"):
        certifications.append("ESS")
    if complements.get("est_societe_mission"):
        certifications.append("Soci√©t√© √† mission")
    if complements.get("est_service_public"):
        certifications.append("Service public")
    certifications_str = ", ".join(certifications) if certifications else "Aucune"
    
    # Conventions collectives
    idcc_list = complements.get("liste_idcc", [])
    idcc_str = ", ".join(idcc_list) if idcc_list else "N/A"
    
    info = {
        # Identification
        "SIRET": siret,
        "SIREN": siren,
        "V√©rification SIREN": siren_verifie,
        "Nom": company_data.get("nom_complet", company_data.get("nom_raison_sociale", "N/A")),
        "Sigle": company_data.get("sigle", "N/A"),
        
        # √âtat et structure
        "√âtat administratif": _format_etat(company_data.get("etat_administratif", "N/A")),
        "Date de cr√©ation": company_data.get("date_creation", "N/A"),
        "Cat√©gorie": company_data.get("categorie_entreprise", "N/A"),
        "Nature juridique": company_data.get("nature_juridique", "N/A"),
        
        # Activit√©
        "Activit√© principale": siege.get("activite_principale", "N/A"),
        "Effectif salari√©": company_data.get("tranche_effectif_salarie", "N/A"),
        "Ann√©e effectif": company_data.get("annee_tranche_effectif_salarie", "N/A"),
        "Nombre d'√©tablissements": company_data.get("nombre_etablissements", "N/A"),
        "√âtablissements ouverts": company_data.get("nombre_etablissements_ouverts", "N/A"),
        
        # Finances
        "Donn√©es financi√®res publi√©es": finances_publiees,
        "Source finances": source_finances,
        "Ann√©e finances": annee_finance,
        "Chiffre d'affaires (CA)": _format_currency(ca),
        "R√©sultat net": _format_currency(resultat_net),
        "Nb exercices (RNE)": nb_exercices_rne if nb_exercices_rne > 0 else "N/A",
        
        # Localisation
        "Adresse si√®ge": siege.get("geo_adresse", siege.get("adresse", "N/A")),
        "Code postal": siege.get("code_postal", "N/A"),
        "Commune": siege.get("libelle_commune", "N/A"),
        "D√©partement": siege.get("departement", "N/A"),
        "R√©gion": siege.get("region", "N/A"),
        "Coordonn√©es GPS": coords,
        
        # Dirigeants
        "Dirigeants": dirigeants_str,
        
        # Certifications et labels
        "Certifications": certifications_str,
        "Conventions collectives (IDCC)": idcc_str,
        
        # Compl√©ments
        "Organisme de formation": "Oui" if complements.get("est_organisme_formation") else "Non",
        "Entrepreneur spectacle": "Oui" if complements.get("est_entrepreneur_spectacle") else "Non",
    }
    
    # Ajouter les colonnes historiques par ann√©e (2019 ‚Üí ann√©e courante)
    import datetime
    current_year = datetime.datetime.now().year
    for year in range(2019, current_year + 1):
        year_str = str(year)
        year_data = historical_data.get(year_str, {})
        info[f"CA {year_str}"] = _format_currency(year_data.get("ca")) if year_data else "N/A"
        info[f"R√©sultat net {year_str}"] = _format_currency(year_data.get("resultat_net")) if year_data else "N/A"
        info[f"R√©sultat exploitation {year_str}"] = _format_currency(year_data.get("resultat_exploitation")) if year_data else "N/A"
        info[f"Effectif {year_str}"] = year_data.get("effectif", "N/A") if year_data else "N/A"
    
    return info


def _format_etat(etat):
    """Format the administrative state."""
    if etat == "A":
        return "Active"
    if etat == "C":
        return "Cess√©e"
    return etat


def _format_currency(value):
    """Format a numeric value as currency."""
    if isinstance(value, (int, float)) and value != "N/A":
        return f"{value:,.0f} ‚Ç¨"
    return value


def process_companies(queries):
    """Process multiple company queries.

    Args:
        queries: List of strings (company names) or tuples (name, siret/siren)
    """
    results = []
    total = len(queries)

    if total > API_IMPORT_MAX_COMPANIES:
        st.warning(
            f"‚ö†Ô∏è Fichier volumineux ({total} lignes). "
            f"Traitement limit√© aux {API_IMPORT_MAX_COMPANIES} premi√®res lignes pour √©viter les quotas API."
        )
        queries = queries[:API_IMPORT_MAX_COMPANIES]
        total = len(queries)

    _init_api_runtime_state()
    st.session_state["api_rate_limit_hits"] = 0
    st.session_state["api_retry_attempts"] = 0
    request_cache = {}

    if USE_API and total > 1:
        estimated_time = total * max(API_DELAY_SECONDS, float(st.session_state.get("api_current_delay", API_DELAY_SECONDS)))
        if estimated_time > 5:
            st.info(f"‚è±Ô∏è {total} entreprise(s) ‚Äî ~{int(estimated_time)}s")

    progress_bar = st.progress(0) if total > 1 else None

    for idx, query_data in enumerate(queries, 1):
        if isinstance(query_data, tuple):
            name, siret_siren = query_data
            query = (
                siret_siren.strip()
                if siret_siren
                and str(siret_siren).strip()
                and str(siret_siren).strip() != "nan"
                else name.strip()
            )
            display_name = (
                name
                if name and str(name).strip() and str(name).strip() != "nan"
                else query
            )
        else:
            query = query_data.strip()
            display_name = query

        if not query:
            continue

        if progress_bar:
            progress_bar.progress(idx / total)

        with st.spinner(f"Recherche {idx}/{total}: {display_name[:40]}..."):
            original_siret = query if is_siret(query) else None

            normalized_key = extract_siren_from_siret(query.strip()) if is_siret(query.strip()) else query.strip().lower()
            if normalized_key in request_cache:
                company_data = request_cache[normalized_key]
            else:
                company_data = search_company_api(query)
                request_cache[normalized_key] = company_data

            rne_data = None

            # Enrich with SQLite finances if available
            if company_data and FINANCES_AVAILABLE:
                siren = company_data.get("siren")
                if siren:
                    rne_data = get_finances(siren)

            if company_data:
                info = extract_financial_info(company_data, original_siret, rne_data)
                results.append(info)
            else:
                results.append(
                    {
                        "SIRET": original_siret or "N/A",
                        "SIREN": extract_siren_from_siret(query)
                        if is_siret(query)
                        else "N/A",
                        "V√©rification SIREN": "‚ùå Non trouv√©",
                        "Nom": f"Non trouv√© ({query})",
                        "√âtat administratif": "N/A",
                        "Cat√©gorie": "N/A",
                        "Nature juridique": "N/A",
                        "Activit√© principale": "N/A",
                        "Effectif salari√©": "N/A",
                        "Nombre d'√©tablissements": "N/A",
                        "Date de cr√©ation": "N/A",
                        "Chiffre d'affaires (CA)": "N/A",
                        "R√©sultat net": "N/A",
                        "Date cl√¥ture exercice": "N/A",
                        "Adresse si√®ge": "N/A",
                    }
                )

    rate_limit_hits = int(st.session_state.get("api_rate_limit_hits", 0))
    retry_attempts = int(st.session_state.get("api_retry_attempts", 0))
    cache_hits = max(0, total - len(request_cache))

    if cache_hits > 0:
        st.info(f"‚ôªÔ∏è D√©duplication activ√©e : {cache_hits} appel(s) API √©vit√©(s)")

    if rate_limit_hits > 0:
        current_delay = float(st.session_state.get("api_current_delay", API_DELAY_SECONDS))
        st.warning(
            f"‚ö†Ô∏è Quota API atteint {rate_limit_hits} fois (r√©essais: {retry_attempts}). "
            f"Cadence adaptative appliqu√©e (d√©lai actuel ‚âà {current_delay:.2f}s)."
        )

    return results


def read_uploaded_file(uploaded_file):
    """Read company data from an uploaded CSV or Excel file.
    
    Returns:
        List of tuples (name, siret_siren) or strings if only one column
    """
    try:
        if uploaded_file.name.endswith('.csv'):
            df = pd.read_csv(uploaded_file, dtype=str)
        elif uploaded_file.name.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(uploaded_file, dtype=str)
        else:
            ext = uploaded_file.name.rsplit('.', 1)[-1] if '.' in uploaded_file.name else '(inconnu)'
            st.error(f"Format de fichier non support√© (.{ext}). "
                     "Utilisez CSV ou Excel (.xlsx/.xls).")
            return []

        # Detect name column
        name_col = None
        for col in df.columns:
            col_lower = col.lower().strip()
            if any(keyword in col_lower for keyword in ['nom', 'name', 'entreprise', 'societe', 'soci√©t√©', 'company', 'raison']):
                name_col = col
                st.info(f"‚úÖ Colonne de noms d√©tect√©e : '{col}'")
                break
        
        # Detect SIRET column
        siret_col = None
        for col in df.columns:
            col_lower = col.lower().strip()
            if 'siret' in col_lower:
                siret_col = col
                st.info(f"‚úÖ Colonne SIRET d√©tect√©e : '{col}'")
                break

        # Detect SIREN column (only if no SIRET)
        siren_col = None
        if siret_col is None:
            for col in df.columns:
                col_lower = col.lower().strip()
                if 'siren' in col_lower:
                    siren_col = col
                    st.info(f"‚úÖ Colonne SIREN d√©tect√©e : '{col}'")
                    break

        # Determine what data we have
        id_col = siret_col or siren_col
        
        if name_col and id_col:
            # Best case: both name and SIRET/SIREN
            st.success(f"üéØ Mode optimal : Noms + SIRET/SIREN d√©tect√©s")
            values = []
            for _, row in df.iterrows():
                name = row[name_col] if pd.notna(row[name_col]) else None
                id_val = row[id_col] if pd.notna(row[id_col]) else None
                # Skip empty rows
                if name or id_val:
                    values.append((str(name) if name else '', str(id_val) if id_val else ''))
            return values
            
        elif name_col:
            # Only names
            st.info(f"üìã Mode : Noms uniquement (colonne '{name_col}')")
            values = df[name_col].dropna().astype(str).str.strip()
            return [v for v in values if v and v != 'nan']
            
        elif id_col:
            # Only SIRET/SIREN
            st.info(f"üìã Mode : SIRET/SIREN uniquement (colonne '{id_col}')")
            values = df[id_col].dropna().astype(str).str.strip()
            return [v for v in values if v and v != 'nan']
            
        else:
            # Fallback: use first column
            first_col = df.columns[0]
            st.warning(f"‚ö†Ô∏è Aucune colonne reconnue. Utilisation de la premi√®re colonne : '{first_col}'")
            values = df[first_col].dropna().astype(str).str.strip()
            return [v for v in values if v and v != 'nan']

        return []
    except Exception as e:
        st.error(f"Erreur lors de la lecture du fichier : {str(e)}")
        return []


def create_download_button(df, file_format, key_suffix=""):
    """Create download button for CSV or XLSX."""
    if file_format == "CSV":
        csv = df.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="üì• T√©l√©charger CSV",
            data=csv,
            file_name="entreprises_donnees_financieres.csv",
            mime="text/csv",
            key=f"dl_csv_{key_suffix}",
        )
    elif file_format == "XLSX":
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='Entreprises')
        output.seek(0)
        st.download_button(
            label="üì• T√©l√©charger XLSX",
            data=output,
            file_name="entreprises_donnees_financieres.xlsx",
            mime="application/vnd.openxmlformats-officedocument."
                 "spreadsheetml.sheet",
            key=f"dl_xlsx_{key_suffix}",
        )


def display_results(results, section_key=""):
    """Display results in a clean, minimal layout."""
    if not results:
        st.info("Aucun r√©sultat.")
        return

    df = pd.DataFrame(results)
    for col in df.columns:
        df[col] = df[col].astype(str)

    verified = sum(1 for r in results if r.get("V√©rification SIREN") == "‚úÖ V√©rifi√©")
    not_found = len(results) - verified
    with_finances = sum(1 for r in results if r.get("Donn√©es financi√®res publi√©es") == "Oui")

    # Compact summary row
    st.markdown("---")
    c1, c2, c3 = st.columns(3)
    c1.metric("Trouv√©es", verified)
    c2.metric("Avec finances", with_finances)
    c3.metric("Non trouv√©es", not_found)

    # Data table
    st.dataframe(df, use_container_width=True, height=min(400, 60 + len(results) * 35))

    # Download row
    col_a, col_b = st.columns(2)
    with col_a:
        create_download_button(df, "CSV", section_key)
    with col_b:
        create_download_button(df, "XLSX", section_key)



# ‚îÄ‚îÄ Main UI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# Section 1: Import de donn√©es
st.markdown("### üì• Import de donn√©es")

# Onglets pour choix du mode d'entr√©e
input_tab1, input_tab2, input_tab3 = st.tabs(["üìÅ Fichier", "üîé Recherche directe", "üéØ Filtres DINUM"])

queries_to_process = None

with input_tab1:
    st.caption("CSV ou Excel ‚Äî colonnes d√©tect√©es automatiquement (nom, SIRET, SIREN)")
    
    uploaded_file = st.file_uploader(
        "Choisissez un fichier",
        type=["csv", "xlsx", "xls"],
        key="main_file_upload"
    )
    
    if uploaded_file is not None:
        queries_to_process = read_uploaded_file(uploaded_file)
        if queries_to_process:
            st.success(f"‚úÖ {len(queries_to_process)} entr√©e(s) d√©tect√©e(s)")

with input_tab2:
    st.caption("Saisissez un nom d'entreprise, un SIREN ou un SIRET")

    direct_query = st.text_input(
        "Entreprise √† rechercher",
        placeholder="Ex: Airbus ou 383474814",
        key="direct_query",
    )

    if direct_query and direct_query.strip():
        queries_to_process = [direct_query.strip()]
        st.success("‚úÖ 1 entreprise pr√™te pour enrichissement")

with input_tab3:
    st.caption("Parcours en 2 √©tapes : comptage puis extraction")

    with st.form("dinum_filters_form"):
        query_filters = st.text_input(
            "Nom / mot-cl√© de recherche (optionnel)",
            placeholder="Ex: transport, informatique, airbus (laisser vide pour pilotage par filtres)",
            key="filters_query",
        )

        col_f1, col_f2 = st.columns(2)
        with col_f1:
            selected_categories = st.multiselect(
                "Cat√©gories d'entreprise",
                options=CATEGORIE_ENTREPRISE_OPTIONS,
                key="filters_categories",
            )
            selected_etat = st.multiselect(
                "√âtat administratif",
                options=ETAT_ADMIN_OPTIONS,
                format_func=lambda x: "Active" if x == "A" else "Cess√©e",
                key="filters_etat",
            )
            selected_effectif = st.multiselect(
                "Tranches effectif salari√©",
                options=TRANCHE_EFFECTIF_OPTIONS,
                key="filters_effectif",
            )

        with col_f2:
            selected_sections = st.multiselect(
                "Sections activit√© principale (NAF)",
                options=SECTION_ACTIVITE_OPTIONS,
                key="filters_sections",
            )
            selected_departements = st.multiselect(
                "D√©partements",
                options=DEPARTEMENT_OPTIONS,
                key="filters_departements",
            )
            selected_regions = st.multiselect(
                "R√©gions (codes)",
                options=REGION_OPTIONS,
                key="filters_regions",
            )

        st.markdown("**Codes et filtres avanc√©s**")
        ape_input = st.text_area(
            "Codes APE/NAF (multis√©lection, s√©par√©s par virgules ou retours ligne)",
            placeholder="62.01Z, 62.02A",
            height=80,
            key="filters_ape_input",
        )
        nj_input = st.text_area(
            "Natures juridiques (codes, multis√©lection)",
            placeholder="5710, 5499",
            height=80,
            key="filters_nature_juridique",
        )
        cp_input = st.text_input(
            "Codes postaux (multis√©lection)",
            placeholder="75001, 69001",
            key="filters_cp",
        )
        commune_input = st.text_input(
            "Codes commune INSEE (multis√©lection)",
            placeholder="75101, 34172",
            key="filters_commune",
        )

        selected_boolean_labels = st.multiselect(
            "Labels / statuts",
            options=list(BOOLEAN_FILTER_LABELS.keys()),
            format_func=lambda x: BOOLEAN_FILTER_LABELS[x],
            key="filters_boolean",
        )

        st.markdown("**Bornes financi√®res (optionnel)**")
        col_num1, col_num2 = st.columns(2)
        with col_num1:
            ca_min = st.number_input("CA min (‚Ç¨)", min_value=0, step=1000, value=0, key="filters_ca_min")
            rn_min = st.number_input("R√©sultat net min (‚Ç¨)", step=1000, value=0, key="filters_rn_min")
        with col_num2:
            ca_max = st.number_input("CA max (‚Ç¨)", min_value=0, step=1000, value=0, key="filters_ca_max")
            rn_max = st.number_input("R√©sultat net max (‚Ç¨)", step=1000, value=0, key="filters_rn_max")

        extraction_limit = st.number_input(
            "Limite d'extraction (protection API/Codespaces)",
            min_value=100,
            max_value=API_EXTRACTION_LIMIT_MAX,
            step=100,
            value=500,
            key="filters_extraction_limit",
        )

        submit_count = st.form_submit_button("1) Compter les entreprises concern√©es", type="primary")

    if submit_count:
        raw_ape_values = _parse_multi_values(ape_input)
        normalized_ape_values, rejected_ape_values = _normalize_naf_codes(raw_ape_values)
        resolved_ape_values, unresolved_ape_values, remapped_ape_values = _resolve_naf_codes_for_api(normalized_ape_values)

        if rejected_ape_values:
            rejected_preview = ", ".join(rejected_ape_values[:5])
            st.warning(
                "‚ö†Ô∏è Certains codes APE/NAF ont √©t√© ignor√©s car invalides : "
                f"{rejected_preview}"
                + (" ..." if len(rejected_ape_values) > 5 else "")
            )

        if remapped_ape_values:
            examples = []
            for original, mapped_values in list(remapped_ape_values.items())[:3]:
                examples.append(f"{original} ‚Üí {', '.join(mapped_values[:3])}")
            st.info(
                "‚ÑπÔ∏è Conversion automatique de codes NAF vers les valeurs accept√©es par l'API : "
                + " | ".join(examples)
            )

        if unresolved_ape_values:
            unresolved_preview = ", ".join(unresolved_ape_values[:5])
            st.warning(
                "‚ö†Ô∏è Certains codes NAF ne correspondent √† aucune valeur API et ont √©t√© ignor√©s : "
                f"{unresolved_preview}"
                + (" ..." if len(unresolved_ape_values) > 5 else "")
            )

        filters_payload = {
            "activite_principale": resolved_ape_values,
            "section_activite_principale": selected_sections,
            "tranche_effectif_salarie": selected_effectif,
            "categorie_entreprise": selected_categories,
            "etat_administratif": selected_etat,
            "departement": selected_departements,
            "region": selected_regions,
            "nature_juridique": _parse_multi_values(nj_input),
            "code_postal": _parse_multi_values(cp_input),
            "code_commune": _parse_multi_values(commune_input),
            "boolean_flags": selected_boolean_labels,
            "ca_min": int(ca_min) if ca_min and ca_min > 0 else None,
            "ca_max": int(ca_max) if ca_max and ca_max > 0 else None,
            "resultat_net_min": int(rn_min) if rn_min and rn_min != 0 else None,
            "resultat_net_max": int(rn_max) if rn_max and rn_max != 0 else None,
        }

        effective_query, query_source = _resolve_effective_query(query_filters, filters_payload)

        if query_source == "naf":
            st.info("‚ÑπÔ∏è Mot-cl√© absent : comptage pilot√© par filtres NAF avec requ√™te g√©n√©rique DINUM.")
        elif query_source == "section_naf":
            st.info("‚ÑπÔ∏è Mot-cl√© absent : comptage pilot√© par section NAF avec requ√™te g√©n√©rique DINUM.")
        elif query_source == "fallback":
            st.warning("‚ö†Ô∏è Mot-cl√© absent et aucun filtre textuel fort d√©tect√© : utilisation de 'entreprise'. Affinez les filtres si le volume est trop large.")

        with st.spinner("Interrogation DINUM pour le comptage..."):
            count_meta = count_companies_api(effective_query, filters_payload)

        if count_meta:
            st.session_state["dinum_filters_query"] = effective_query
            st.session_state["dinum_filters_query_source"] = query_source
            st.session_state["dinum_filters_payload"] = filters_payload
            st.session_state["dinum_filters_count"] = count_meta["total_results"]
            st.session_state["dinum_filters_limit"] = int(extraction_limit)
            st.session_state.pop("dinum_filters_companies", None)
            st.session_state.pop("dinum_filters_results", None)
            st.success("‚úÖ Comptage termin√©")

    if "dinum_filters_count" in st.session_state:
        total_count = st.session_state["dinum_filters_count"]
        extraction_limit = st.session_state.get("dinum_filters_limit", 500)
        effective_query = st.session_state.get("dinum_filters_query", "")
        query_source = st.session_state.get("dinum_filters_query_source", "manual")

        if effective_query:
            query_label = "Mot-cl√©" if query_source == "manual" else "Requ√™te auto"
            st.caption(f"{query_label} utilis√© : {effective_query}")

        c_count1, c_count2 = st.columns(2)
        c_count1.metric("Entreprises correspondant aux filtres", f"{total_count:,}".replace(",", " "))
        c_count2.metric("Extraction max pr√©vue", f"{extraction_limit:,}".replace(",", " "))

        if total_count > extraction_limit:
            st.warning(
                f"‚ö†Ô∏è Volum√©trie √©lev√©e : {total_count:,} r√©sultats. "
                f"L'extraction sera limit√©e aux {extraction_limit:,} premiers r√©sultats "
                "pour respecter les capacit√©s API/Codespaces."
                .replace(",", " ")
            )

        if st.button("2) Valider et extraire les donn√©es DINUM", type="secondary", use_container_width=True, key="btn_extract_filtered"):
            with st.spinner("Extraction DINUM en cours..."):
                raw_companies, total_found = fetch_companies_api(
                    st.session_state["dinum_filters_query"],
                    st.session_state["dinum_filters_payload"],
                    st.session_state["dinum_filters_limit"],
                )

            if raw_companies:
                st.session_state["dinum_filters_companies"] = raw_companies
                st.session_state["dinum_filters_total_found"] = total_found
                st.session_state["dinum_filters_results"] = companies_to_results(raw_companies, use_rne=False)
                st.success(
                    f"‚úÖ Extraction termin√©e : {len(raw_companies)} entreprise(s) r√©cup√©r√©e(s)"
                )
            else:
                st.warning("Aucune entreprise extraite avec ces filtres.")

    if "dinum_filters_results" in st.session_state and st.session_state["dinum_filters_results"]:
        st.markdown("#### R√©sultats DINUM extraits")
        display_results(st.session_state["dinum_filters_results"], section_key="dinum_filters")

        can_rne = FINANCES_AVAILABLE and db_available()
        if st.button(
            "3) Enrichir ces r√©sultats avec la base RNE",
            type="secondary",
            use_container_width=True,
            key="btn_rne_after_filters",
            disabled=not can_rne,
        ):
            raw_companies = st.session_state.get("dinum_filters_companies", [])
            if not raw_companies:
                st.warning("Aucune donn√©e brute disponible pour l'enrichissement RNE.")
            else:
                with st.spinner("Enrichissement RNE en cours..."):
                    st.session_state["dinum_filters_results"] = companies_to_results(raw_companies, use_rne=True)
                st.success("‚úÖ Enrichissement RNE termin√©")

# Section 2: Choix de la source de donn√©es
st.markdown("---")
st.markdown("### ‚öôÔ∏è Source des donn√©es")

# V√©rifier la disponibilit√© de Pappers
try:
    from enrichment_pappers import check_api_key, SCRAPING_ENABLED
    PAPPERS_AVAILABLE = check_api_key() or SCRAPING_ENABLED
except ImportError:
    PAPPERS_AVAILABLE = False

# Options disponibles
col1, col2, col3 = st.columns(3)

with col1:
    if st.button("üèõÔ∏è DINUM", use_container_width=True, help="API officielle de l'√âtat", type="secondary"):
        st.session_state["data_source"] = "dinum"

with col2:
    rne_help = "Base locale RNE/INPI" if FINANCES_AVAILABLE and db_available() else "Base RNE non disponible"
    rne_disabled = not (FINANCES_AVAILABLE and db_available())
    if st.button("üìä RNE", use_container_width=True, help=rne_help, disabled=rne_disabled, type="secondary"):
        st.session_state["data_source"] = "rne"

with col3:
    pappers_help = "RNE + enrichissement Pappers" if PAPPERS_AVAILABLE else "Pappers non configur√©"
    pappers_disabled = not PAPPERS_AVAILABLE
    if st.button("üí∞ RNE + Pappers", use_container_width=True, help=pappers_help, disabled=pappers_disabled, type="secondary"):
        st.session_state["data_source"] = "rne_pappers"

# Afficher la source s√©lectionn√©e
if "data_source" in st.session_state:
    source_labels = {
        "dinum": "üèõÔ∏è DINUM (API officielle)",
        "rne": "üìä RNE (Base locale)",
        "rne_pappers": "üí∞ RNE + Pappers (Enrichissement complet)"
    }
    st.info(f"**Source active :** {source_labels.get(st.session_state['data_source'], 'Non s√©lectionn√©e')}")
else:
    st.warning("‚ö†Ô∏è S√©lectionnez une source de donn√©es ci-dessus")

# Section 3: Bouton de recherche (actif seulement si donn√©es + source)
st.markdown("---")

if queries_to_process and "data_source" in st.session_state:
    if st.button("üîç Lancer l'enrichissement", type="primary", use_container_width=True, key="btn_enrich"):
        with st.spinner("Traitement en cours..."):
            # Traiter selon la source s√©lectionn√©e
            if st.session_state["data_source"] == "dinum":
                results = process_companies(queries_to_process)
            elif st.session_state["data_source"] == "rne":
                results = process_companies(queries_to_process)  # Utilise d√©j√† RNE automatiquement
            elif st.session_state["data_source"] == "rne_pappers":
                # TODO: Int√©grer l'enrichissement Pappers
                st.warning("‚ö†Ô∏è L'enrichissement Pappers sera int√©gr√© prochainement")
                results = process_companies(queries_to_process)
            
            st.session_state["results_main"] = results
            st.success("‚úÖ Enrichissement termin√© !")
    
    # Afficher les r√©sultats persist√©s
    if "results_main" in st.session_state and st.session_state["results_main"]:
        display_results(st.session_state["results_main"], section_key="main")
elif not queries_to_process:
    st.info("üì§ Importez un fichier ou saisissez des entreprises pour commencer")
elif "data_source" not in st.session_state:
    st.info("‚öôÔ∏è S√©lectionnez une source de donn√©es")

# ‚îÄ‚îÄ Sidebar (minimal) ‚îÄ‚îÄ

with st.sidebar:
    st.markdown("### ‚ÑπÔ∏è √Ä propos")
    st.caption("Outil d'enrichissement de donn√©es d'entreprises fran√ßaises")

    st.markdown("---")
    st.markdown("### üìä Sources de donn√©es")
    
    # DINUM (toujours disponible)
    st.markdown("**üèõÔ∏è DINUM**")
    st.caption("‚úÖ API officielle de l'√âtat")
    st.caption("‚Üí Donn√©es d'identification et l√©gales")
    
    # RNE (base locale)
    st.markdown("**üìä RNE / INPI**")
    if FINANCES_AVAILABLE and db_available():
        st.caption("‚úÖ Base SQLite disponible")
        age = db_age_days()
        if age is not None:
            if age > DB_AGE_WARNING_DAYS:
                st.caption(f"‚ö†Ô∏è Mise √† jour : il y a {age} jours")
            else:
                st.caption(f"üìÖ Mise √† jour : il y a {age} jours")
        st.caption("‚Üí Donn√©es financi√®res locales")
    else:
        st.caption("‚ùå Base non disponible")
        st.caption("‚Üí Ex√©cutez `python build_rne_db.py`")
    
    # Pappers
    st.markdown("**üí∞ Pappers.fr**")
    try:
        from enrichment_pappers import check_api_key, SCRAPING_ENABLED
        has_api = check_api_key()
        has_scraping = SCRAPING_ENABLED
        
        if has_api:
            st.caption("‚úÖ API configur√©e")
            st.caption("‚Üí Enrichissement complet disponible")
        elif has_scraping:
            st.caption("‚ö†Ô∏è Mode scraping activ√©")
            st.caption("‚Üí Plus lent mais gratuit")
        else:
            st.caption("‚ùå Non configur√©")
            st.caption("‚Üí Ajoutez une cl√© dans `.env`")
    except ImportError:
        st.caption("‚ùå Module non disponible")

    st.markdown("---")
    st.markdown("### ‚è±Ô∏è Cadence API")
    current_delay = float(st.session_state.get("api_current_delay", API_DELAY_SECONDS)) if "api_current_delay" in st.session_state else API_DELAY_SECONDS
    st.caption(f"D√©lai de base : {API_DELAY_SECONDS:.2f}s")
    st.caption(f"D√©lai actuel : {current_delay:.2f}s")
    st.caption(f"Limite import : {API_IMPORT_MAX_COMPANIES} lignes")

    st.markdown("---")
    st.caption("‚ö†Ô∏è 10-20 % des entreprises publient leurs comptes")
    st.markdown("[Documentation API](https://recherche-entreprises.api.gouv.fr/)")
