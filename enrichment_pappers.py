"""
Module d'enrichissement des donn√©es d'entreprises via l'API Pappers.fr
R√©cup√®re l'historique financier complet et donn√©es suppl√©mentaires

Deux modes disponibles:
1. API (recommand√©) - N√©cessite cl√© API
2. Scraping (fallback) - Gratuit mais plus lent et fragile
"""

import os
import time
import random
import requests
import pandas as pd
from typing import Dict, List, Optional, Any
from dotenv import load_dotenv
from bs4 import BeautifulSoup
import re

# Charger les variables d'environnement
load_dotenv()

# Configuration
PAPPERS_API_KEY = os.getenv('PAPPERS_API_KEY', '')
PAPPERS_DELAY = float(os.getenv('PAPPERS_DELAY_SECONDS', '0.5'))
PAPPERS_BASE_URL = "https://api.pappers.fr/v2"
PAPPERS_WEB_URL = "https://www.pappers.fr/entreprise"
PAPPERS_MAX_RETRIES = 3

# Configuration scraping
SCRAPING_MIN_DELAY = float(os.getenv('SCRAPING_MIN_DELAY', '2.0'))
SCRAPING_MAX_DELAY = float(os.getenv('SCRAPING_MAX_DELAY', '5.0'))
SCRAPING_ENABLED = os.getenv('SCRAPING_ENABLED', 'true').lower() == 'true'

# User agents pour rotation
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
]


def check_api_key() -> bool:
    """V√©rifie si la cl√© API Pappers est configur√©e"""
    return bool(PAPPERS_API_KEY and PAPPERS_API_KEY != 'votre_cle_api_ici')


def get_random_delay() -> float:
    """G√©n√®re un d√©lai al√©atoire pour le scraping"""
    return random.uniform(SCRAPING_MIN_DELAY, SCRAPING_MAX_DELAY)


def get_random_headers() -> Dict[str, str]:
    """G√©n√®re des headers HTTP al√©atoires pour √©viter la d√©tection"""
    return {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Cache-Control': 'max-age=0',
    }


def get_company_data_pappers(siren: str) -> Optional[Dict[str, Any]]:
    """
    R√©cup√®re les donn√©es compl√®tes d'une entreprise via l'API Pappers
    
    Args:
        siren: Num√©ro SIREN de l'entreprise (9 chiffres)
        
    Returns:
        Dictionnaire avec les donn√©es de l'entreprise ou None en cas d'erreur
    """
    if not check_api_key():
        return None
    
    # Nettoyer le SIREN
    siren = str(siren).strip().replace(' ', '')[:9]
    if not siren.isdigit() or len(siren) != 9:
        return None
    
    url = f"{PAPPERS_BASE_URL}/entreprise"
    params = {
        'api_token': PAPPERS_API_KEY,
        'siren': siren,
        'format_publications_bodacc': 'true'
    }
    
    for attempt in range(PAPPERS_MAX_RETRIES):
        try:
            response = requests.get(url, params=params, timeout=30)
            
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 429:
                # Rate limit - attendre plus longtemps
                wait_time = PAPPERS_DELAY * (2 ** attempt)
                time.sleep(wait_time)
                continue
            elif response.status_code == 404:
                # Entreprise non trouv√©e
                return None
            else:
                # Autre erreur
                if attempt < PAPPERS_MAX_RETRIES - 1:
                    time.sleep(PAPPERS_DELAY)
                    continue
                return None
                
        except requests.exceptions.RequestException:
            if attempt < PAPPERS_MAX_RETRIES - 1:
                time.sleep(PAPPERS_DELAY)
                continue
            return None
    
    return None


def scrape_company_data_pappers(siren: str) -> Optional[Dict[str, Any]]:
    """
    R√©cup√®re les donn√©es d'une entreprise par scraping web de Pappers.fr
    Fallback gratuit quand l'API n'est pas disponible
    
    Args:
        siren: Num√©ro SIREN de l'entreprise (9 chiffres)
        
    Returns:
        Dictionnaire avec les donn√©es financi√®res ou None en cas d'erreur
    """
    if not SCRAPING_ENABLED:
        return None
    
    # Nettoyer le SIREN
    siren = str(siren).strip().replace(' ', '')[:9]
    if not siren.isdigit() or len(siren) != 9:
        return None
    
    # URL de la page entreprise
    url = f"{PAPPERS_WEB_URL}/{siren}"
    
    for attempt in range(PAPPERS_MAX_RETRIES):
        try:
            # D√©lai al√©atoire avant la requ√™te
            if attempt > 0:
                time.sleep(get_random_delay())
            
            # Requ√™te HTTP avec headers al√©atoires
            headers = get_random_headers()
            response = requests.get(url, headers=headers, timeout=30)
            
            if response.status_code == 200:
                # Parser le HTML
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Extraire les donn√©es financi√®res
                finances = []
                
                # Chercher la section des finances
                # Pappers.fr structure: tables avec class="financials" ou similaire
                finance_sections = soup.find_all(['table', 'div'], class_=re.compile(r'financ|bilan|compte', re.I))
                
                for section in finance_sections:
                    # Extraire les lignes de donn√©es
                    rows = section.find_all('tr')
                    
                    current_year_data = {}
                    current_year = None
                    
                    for row in rows:
                        cells = row.find_all(['td', 'th'])
                        if len(cells) >= 2:
                            label = cells[0].get_text(strip=True).lower()
                            value_text = cells[1].get_text(strip=True)
                            
                            # D√©tecter l'ann√©e
                            year_match = re.search(r'20\d{2}', value_text)
                            if year_match:
                                current_year = year_match.group()
                            
                            # Extraire les valeurs num√©riques
                            value_match = re.search(r'([\d\s]+)', value_text.replace(' ', ''))
                            if value_match:
                                try:
                                    value = int(value_match.group(1).replace(' ', ''))
                                    
                                    # Identifier le type de donn√©e
                                    if 'chiffre' in label or 'ca' in label:
                                        current_year_data['ca'] = value
                                    elif 'r√©sultat' in label and 'net' in label:
                                        current_year_data['resultat_net'] = value
                                    elif 'effectif' in label:
                                        current_year_data['effectif'] = value
                                except ValueError:
                                    continue
                    
                    if current_year and current_year_data:
                        finances.append({
                            'date_cloture_exercice': f'{current_year}-12-31',
                            **current_year_data
                        })
                
                # Alternative: chercher les donn√©es dans le JSON embarqu√©
                scripts = soup.find_all('script', type='application/ld+json')
                for script in scripts:
                    try:
                        import json
                        data = json.loads(script.string)
                        if isinstance(data, dict) and 'finances' in data:
                            return {'finances': data['finances']}
                    except:
                        continue
                
                if finances:
                    return {'finances': finances}
                else:
                    return None
                    
            elif response.status_code == 404:
                return None
            elif response.status_code == 429:
                # Rate limit - attendre plus longtemps
                wait_time = get_random_delay() * (2 ** attempt)
                time.sleep(wait_time)
                continue
            else:
                if attempt < PAPPERS_MAX_RETRIES - 1:
                    time.sleep(get_random_delay())
                    continue
                return None
                
        except requests.exceptions.RequestException:
            if attempt < PAPPERS_MAX_RETRIES - 1:
                time.sleep(get_random_delay())
                continue
            return None
    
    return None


def get_company_data_unified(siren: str, prefer_api: bool = True) -> Optional[Dict[str, Any]]:
    """  
    R√©cup√®re les donn√©es d'entreprise avec fallback automatique API ‚Üí Scraping
    
    Args:
        siren: Num√©ro SIREN de l'entreprise
        prefer_api: Si True, essaye l'API d'abord puis scraping. Si False, scraping uniquement.
        
    Returns:
        Dictionnaire avec les donn√©es ou None
    """
    # Tentative 1: API (si cl√© disponible et pr√©f√©r√©e)
    if prefer_api and check_api_key():
        data = get_company_data_pappers(siren)
        if data:
            return data
    
    # Tentative 2: Scraping (si activ√©)
    if SCRAPING_ENABLED:
        data = scrape_company_data_pappers(siren)
        if data:
            return data
    
    return None


def extract_financial_history(pappers_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Extrait l'historique financier complet depuis les donn√©es Pappers
    
    Returns:
        Liste de dictionnaires avec ann√©e, CA, r√©sultat net, effectif, etc.
    """
    history = []
    
    if not pappers_data or 'finances' not in pappers_data:
        return history
    
    for finance in pappers_data.get('finances', []):
        year_data = {
            'annee': finance.get('date_cloture_exercice', '')[:4],
            'ca': finance.get('chiffre_affaires'),
            'resultat_net': finance.get('resultat'),
            'effectif': finance.get('effectif'),
            'resultat_exploitation': finance.get('resultat_exploitation'),
            'excedent_brut_exploitation': finance.get('excedent_brut_exploitation'),
            'capacite_autofinancement': finance.get('capacite_autofinancement'),
            'fonds_roulement': finance.get('fonds_roulement'),
            'dette_financiere': finance.get('dette_financiere'),
            'marge_brute': finance.get('marge_brute'),
            'duree_exercice_mois': finance.get('duree_exercice')
        }
        history.append(year_data)
    
    # Trier par ann√©e d√©croissante
    history.sort(key=lambda x: x.get('annee', ''), reverse=True)
    return history


def format_financial_data(history: List[Dict[str, Any]], prefix: str = '') -> Dict[str, str]:
    """
    Formate les donn√©es financi√®res pour l'export Excel
    Cr√©e des colonnes distinctes pour chaque ann√©e
    
    Args:
        history: Liste des donn√©es financi√®res par ann√©e
        prefix: Pr√©fixe pour les noms de colonnes (ex: 'Pappers_')
        
    Returns:
        Dictionnaire avec colonnes format√©es
    """
    formatted = {}
    
    for i, year_data in enumerate(history[:10]):  # Max 10 ann√©es
        year = year_data.get('annee', f'N-{i}')
        
        # Chiffre d'affaires
        ca = year_data.get('ca')
        formatted[f'{prefix}CA_{year}'] = _format_currency(ca) if ca else ''
        
        # R√©sultat net
        resultat = year_data.get('resultat_net')
        formatted[f'{prefix}Resultat_{year}'] = _format_currency(resultat) if resultat else ''
        
        # Effectif
        effectif = year_data.get('effectif')
        formatted[f'{prefix}Effectif_{year}'] = str(effectif) if effectif else ''
    
    # Statistiques sur l'historique
    if history:
        formatted[f'{prefix}Annees_Disponibles'] = len(history)
        formatted[f'{prefix}Derniere_Annee'] = history[0].get('annee', '')
    else:
        formatted[f'{prefix}Annees_Disponibles'] = 0
        formatted[f'{prefix}Derniere_Annee'] = ''
    
    return formatted


def _format_currency(value: Optional[float]) -> str:
    """Formate une valeur mon√©taire en euros"""
    if value is None:
        return ""
    try:
        return f"{int(value):,} ‚Ç¨".replace(',', ' ')
    except (ValueError, TypeError):
        return ""


def enrich_with_pappers(df: pd.DataFrame, siren_column: str = 'SIREN') -> pd.DataFrame:
    """
    Enrichit un DataFrame avec les donn√©es financi√®res de Pappers
    
    Args:
        df: DataFrame contenant au minimum une colonne SIREN
        siren_column: Nom de la colonne contenant les SIREN
        
    Returns:
        DataFrame enrichi avec les donn√©es Pappers
    """
    if not check_api_key():
        raise ValueError(
            "‚ùå Cl√© API Pappers non configur√©e.\n"
            "Cr√©ez un fichier .env avec PAPPERS_API_KEY=votre_cl√©\n"
            "Obtenez une cl√© sur: https://www.pappers.fr/api"
        )
    
    if siren_column not in df.columns:
        raise ValueError(f"Colonne '{siren_column}' introuvable dans le DataFrame")
    
    # Pr√©parer la liste d'enrichissement
    enriched_data = []
    total = len(df)
    
    print(f"\nüîç Enrichissement Pappers.fr de {total} entreprises...")
    print(f"‚è±Ô∏è  D√©lai entre requ√™tes: {PAPPERS_DELAY}s")
    
    start_time = time.time()
    
    for idx, row in df.iterrows():
        siren = str(row.get(siren_column, '')).strip()
        
        if not siren or siren == 'nan':
            enriched_data.append({})
            continue
        
        # Appel API Pappers avec fallback scraping
        pappers_data = get_company_data_unified(siren, prefer_api=True)
        
        if pappers_data:
            # Extraire l'historique financier
            history = extract_financial_history(pappers_data)
            formatted = format_financial_data(history, prefix='Pappers_')
            enriched_data.append(formatted)
            
            # Afficher progression
            if (idx + 1) % 10 == 0 or (idx + 1) == total:
                elapsed = time.time() - start_time
                rate = (idx + 1) / elapsed if elapsed > 0 else 0
                remaining = (total - idx - 1) / rate if rate > 0 else 0
                print(f"  ‚úì {idx + 1}/{total} - {len(history)} ann√©es trouv√©es pour {siren} - "
                      f"Temps restant: ~{int(remaining)}s")
        else:
            enriched_data.append({})
        
        # Respecter le rate limit
        if idx < total - 1:
            # Si on utilise le scraping, d√©lai al√©atoire
            if not check_api_key() or pappers_data is None:
                delay = get_random_delay()
            else:
                delay = PAPPERS_DELAY
            time.sleep(delay)
    
    # Cr√©er DataFrame avec les nouvelles colonnes
    enriched_df = pd.DataFrame(enriched_data)
    
    # Fusionner avec le DataFrame original
    result_df = pd.concat([df.reset_index(drop=True), enriched_df], axis=1)
    
    # Statistiques
    has_pappers_data = enriched_df.get('Pappers_Annees_Disponibles', pd.Series([0])) > 0
    success_count = has_pappers_data.sum()
    success_rate = (success_count / total * 100) if total > 0 else 0
    
    elapsed = time.time() - start_time
    print(f"\n‚úÖ Enrichissement termin√© en {int(elapsed)}s")
    print(f"üìä Donn√©es Pappers trouv√©es: {success_count}/{total} ({success_rate:.1f}%)")
    
    return result_df


def main():
    """Fonction de test du module"""
    has_api = check_api_key()
    
    if not has_api and not SCRAPING_ENABLED:
        print("‚ùå Ni API ni scraping configur√©s")
        print("Cr√©ez un fichier .env avec:")
        print("PAPPERS_API_KEY=votre_cl√©_ici")
        print("ou")
        print("SCRAPING_ENABLED=true")
        return
    
    if has_api:
        print("‚úÖ Mode: API Pappers (avec fallback scraping)")
    else:
        print("‚ö†Ô∏è  Mode: Scraping uniquement (API non configur√©e)")
        print(f"üìä D√©lais al√©atoires: {SCRAPING_MIN_DELAY}s - {SCRAPING_MAX_DELAY}s")
    
    # Test avec quelques SIREN
    test_sirens = [
        '449162163',  # CISCO SYSTEMS CAPITAL FRANCE
        '552100554',  # CARREFOUR
        '542065479'   # ORANGE
    ]
    
    print("üß™ Test du module d'enrichissement Pappers\n")
    
    for siren in test_sirens:
        print(f"\nüìä Test SIREN: {siren}")
        data = get_company_data_unified(siren)
        
        if data:
            history = extract_financial_history(data)
            print(f"  ‚úì {len(history)} ann√©es de donn√©es financi√®res")
            
            if history:
                latest = history[0]
                print(f"  üìÖ Derni√®re ann√©e: {latest.get('annee')}")
                if latest.get('ca'):
                    print(f"  üí∞ CA: {_format_currency(latest.get('ca'))}")
                if latest.get('resultat_net'):
                    print(f"  üìà R√©sultat: {_format_currency(latest.get('resultat_net'))}")
        else:
            print("  ‚ùå Donn√©es non trouv√©es")
        
        time.sleep(PAPPERS_DELAY)


if __name__ == '__main__':
    main()
