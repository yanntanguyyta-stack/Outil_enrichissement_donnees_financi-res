#!/usr/bin/env python3
"""
Module d'enrichissement RNE optimis√© avec index par ranges
Combine API DINUM (recherche) + FTP RNE (donn√©es financi√®res)

STOCKAGE: ~50 KB seulement !
"""

import json
import zipfile
import io
from pathlib import Path
from ftplib import FTP
from typing import Dict, List, Optional, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# Configuration
FTP_HOST = "www.inpi.net"
FTP_USER = "rneinpiro"
FTP_PASSWORD = "vv8_rQ5f4M_2-E"
FTP_ZIP_FILE = "stock_RNE_comptes_annuels_20250926_1000_v2.zip"

INDEX_FILE = Path("/workspaces/TestsMCP/rne_siren_ranges.json")
CACHE_DIR = Path("/workspaces/TestsMCP/rne_cache")

# Configuration traitement par lots
MAX_CONCURRENT_FILES = 1  # 1 seul fichier √† la fois pour √©conomiser l'espace
AVG_RNE_FILE_SIZE_MB = 80  # Taille r√©elle moyenne d'un fichier RNE

# Timeouts et retry
FTP_TIMEOUT = 60  # Timeout FTP en secondes
FTP_MAX_RETRIES = 2  # Nombre de tentatives
ZIP_DOWNLOAD_WARNING_SHOWN = False  # Flag pour √©viter les avertissements r√©p√©t√©s

# Mode espace disque limit√©
LIMITED_SPACE_MODE = True  # Activer le nettoyage agressif apr√®s chaque fichier

# Mode filtrage des donn√©es (ne garder que les donn√©es financi√®res)
FINANCIAL_ONLY_MODE = False  # D√âSACTIV√â: gain seulement 2%, pas assez significatif
# Pour un vrai gain (99%), il faudrait extraire seulement les 6 indicateurs cl√©s  
# mais on perdrait la flexibilit√© d'ajouter d'autres indicateurs plus tard

# Mode Streaming (extraction √† la vol√©e des indicateurs cl√©s)
STREAMING_MODE = True  # Extraire seulement les 6 indicateurs sans tout charger en m√©moire
# Avantages: Fichiers 99% plus petits (~80 MB ‚Üí ~1 KB), parsing instantan√©, m√©moire minimale
# Inconv√©nient: Pas de flexibilit√© pour ajouter d'autres indicateurs plus tard

# Codes de liasse principaux
LIASSE_CODES = {
    "FA": "Chiffre d'affaires",
    "HN": "R√©sultat net",
    "GC": "R√©sultat d'exploitation",
    "BJ": "Total actif",
    "DL": "Capitaux propres",
    "HY": "Effectif moyen",
}


def load_ranges_index() -> Optional[Dict]:
    """Charger l'index des ranges SIREN"""
    if not INDEX_FILE.exists():
        print(f"‚ö†Ô∏è Index non trouv√©: {INDEX_FILE}")
        return None
    
    with open(INDEX_FILE, 'r') as f:
        return json.load(f)


def extract_key_metrics_only(data: List[Dict]) -> List[Dict]:
    """
    MODE STREAMING: Extraire SEULEMENT les 6 indicateurs financiers cl√©s.
    
    Au lieu de garder tout bilanSaisi (80 MB par fichier), on extrait seulement :
    - FA: Chiffre d'affaires
    - HN: R√©sultat net  
    - GC: R√©sultat d'exploitation
    - BJ: Total actif
    - DL: Capitaux propres
    - HY: Effectif moyen
    
    Structure d'origine (bilanSaisi.detail.pages[].liasses[]):
    {
      "bilanSaisi": {
        "detail": {
          "pages": [
            {"numero": 1, "liasses": [{"code": "FA", "m1": "1234", "m2": "5678"}, ...]},
            {"numero": 2, "liasses": [{"code": "HN", "m1": "9012", "m2": "3456"}, ...]}
          ]
        }
      }
    }
    
    Gain: Fichier de 83 MB ‚Üí ~500 KB (99% de r√©duction !)
          Parsing quasi instantan√©
          M√©moire minimale
    """
    if not STREAMING_MODE:
        return data  # Mode d√©sactiv√©, garder tout
    
    filtered = []
    codes_recherches = set(LIASSE_CODES.keys())  # FA, HN, GC, BJ, DL, HY
    
    for entreprise in data:
        bilan_saisi = entreprise.get("bilanSaisi", {})
        bilan = bilan_saisi.get("bilan", {})  # NIVEAU SUPPL√âMENTAIRE
        detail = bilan.get("detail", {})
        pages = detail.get("pages", [])
        
        # Extraire seulement les codes qui nous int√©ressent
        metrics = {}
        for page in pages:
            liasses = page.get("liasses", [])
            for liasse in liasses:
                code = liasse.get("code")
                if code in codes_recherches:
                    # Garder m1 (ann√©e N) et m2 (ann√©e N-1) si disponibles
                    metrics[code] = {
                        "m1": liasse.get("m1"),
                        "m2": liasse.get("m2")
                    }
        
        # Structure ultra-l√©g√®re
        filtered_entreprise = {
            "siren": entreprise.get("siren"),
            "dateCloture": entreprise.get("dateCloture"),
            "dateDepot": entreprise.get("dateDepot"),
            "typeBilan": entreprise.get("typeBilan"),
            "metrics": metrics  # Seulement les 6 indicateurs
        }
        filtered.append(filtered_entreprise)
    
    return filtered


def filter_financial_data_only(data: List[Dict]) -> List[Dict]:
    """
    Filtrer les donn√©es RNE pour ne garder QUE les donn√©es financi√®res.
    
    R√©duit la taille de ~90% en supprimant :
    - denomination, adresse, forme juridique, etc. (d√©j√† dans API Pappers/DINUM)
    - M√©tadonn√©es techniques (id, updatedAt, deleted, etc.)
    
    Garde uniquement :
    - siren : pour identifier l'entreprise
    - dateCloture : date de cl√¥ture de l'exercice
    - dateDepot : date de d√©p√¥t des comptes
    - bilanSaisi : LES DONN√âES COMPTABLES (l'essentiel !)
    - typeBilan : type de bilan (C=consolid√©, S=simplifi√©, etc.)
    
    Gain: Fichier de 83 MB ‚Üí ~8 MB (10x plus petit)
          M√©moire divis√©e par 10
          Parsing 10x plus rapide
    """
    if not FINANCIAL_ONLY_MODE:
        return data  # Mode d√©sactiv√©, garder tout
    
    filtered = []
    for entreprise in data:
        # Ne garder QUE les donn√©es financi√®res essentielles
        filtered_entreprise = {
            "siren": entreprise.get("siren"),
            "dateCloture": entreprise.get("dateCloture"),
            "dateDepot": entreprise.get("dateDepot"),
            "bilanSaisi": entreprise.get("bilanSaisi"),
            "typeBilan": entreprise.get("typeBilan"),
        }
        filtered.append(filtered_entreprise)
    
    return filtered


def load_ranges_index() -> Optional[Dict]:
    """Charger l'index des ranges de SIRENs"""
    if not INDEX_FILE.exists():
        print(f"‚ùå Index non trouv√©: {INDEX_FILE}")
        print(f"üí° Cr√©ez-le avec: python3 create_rne_index_ranges.py")
        return None
    
    with open(INDEX_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)


def find_file_for_siren(siren: str, ranges: List[Dict]) -> Optional[str]:
    """
    Trouver le fichier contenant un SIREN par recherche binaire dans les ranges
    Tr√®s rapide: O(log n) au lieu de O(1) mais index 1000x plus petit
    """
    siren = str(siren).zfill(9)
    
    # Recherche binaire
    left, right = 0, len(ranges) - 1
    
    while left <= right:
        mid = (left + right) // 2
        r = ranges[mid]
        
        if r['siren_min'] <= siren <= r['siren_max']:
            return r['file']
        elif siren < r['siren_min']:
            right = mid - 1
        else:
            left = mid + 1
    
    return None


def download_json_from_ftp(filename: str, use_cache: bool = True) -> Optional[List[Dict]]:
    """T√©l√©charger un fichier JSON depuis le FTP
    
    ‚ö†Ô∏è  ATTENTION: Cette fonction t√©l√©charge le ZIP complet (3.5 GB) √† chaque fois.
    Pour √©viter cela, t√©l√©chargez le ZIP une seule fois et extrayez localement.
    """
    global ZIP_DOWNLOAD_WARNING_SHOWN
    
    cache_path = CACHE_DIR / filename
    
    # Cache
    if use_cache and cache_path.exists():
        print(f"üìÇ Cache: {filename}")
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Si le fichier en cache n'est pas dans le bon format, le convertir
            if data and len(data) > 0:
                first_item = data[0]
                
                # Mode Streaming : v√©rifier si le cache est au format streaming
                if STREAMING_MODE:
                    if 'bilanSaisi' in first_item:  # Ancien format (complet)
                        print(f"   üóúÔ∏è  Conversion en mode Streaming (ancien format d√©tect√©)...")
                        data = extract_key_metrics_only(data)
                        # Sauvegarder la version streaming
                        with open(cache_path, 'w', encoding='utf-8') as f:
                            json.dump(data, f)
                
                # Mode Filtrage : v√©rifier si le cache est filtr√©
                elif FINANCIAL_ONLY_MODE and ('denomination' in first_item or 'id' in first_item):
                    print(f"   üóúÔ∏è  Filtrage du cache (ancien format d√©tect√©)...")
                    data = filter_financial_data_only(data)
                    # Sauvegarder la version filtr√©e
                    with open(cache_path, 'w', encoding='utf-8') as f:
                        json.dump(data, f)
            
            return data
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur lecture cache {filename}: {e}")
            # Continuer avec t√©l√©chargement
    
    # Avertissement sur le t√©l√©chargement complet
    if not ZIP_DOWNLOAD_WARNING_SHOWN:
        print(f"\n‚ö†Ô∏è  AVERTISSEMENT: T√©l√©chargement du ZIP complet (3.5 GB)")
        print(f"üí° Pour de meilleures performances, t√©l√©chargez le ZIP une fois:")
        print(f"   wget ftp://{FTP_USER}:{FTP_PASSWORD}@{FTP_HOST}/{FTP_ZIP_FILE}")
        print(f"   Puis extrayez tous les fichiers localement\n")
        ZIP_DOWNLOAD_WARNING_SHOWN = True
    
    # T√©l√©charger avec retry
    for attempt in range(FTP_MAX_RETRIES):
        try:
            print(f"‚¨áÔ∏è  FTP: {filename} (Tentative {attempt + 1}/{FTP_MAX_RETRIES})")
            print(f"   ‚è±Ô∏è  Cela peut prendre 30-60s (t√©l√©chargement 3.5 GB)...")
            
            ftp = FTP(FTP_HOST, timeout=FTP_TIMEOUT)
            ftp.login(FTP_USER, FTP_PASSWORD)
            
            # T√©l√©charger le ZIP complet
            zip_buffer = io.BytesIO()
            
            def progress_callback(block):
                # Afficher progression tous les 100 MB
                if zip_buffer.tell() % (100 * 1024 * 1024) < 8192:
                    mb_downloaded = zip_buffer.tell() / (1024 * 1024)
                    print(f"   üì• {mb_downloaded:.0f} MB t√©l√©charg√©s...")
            
            ftp.retrbinary(f'RETR {FTP_ZIP_FILE}', zip_buffer.write, blocksize=8192)
            ftp.quit()
            
            print(f"   ‚úÖ T√©l√©chargement termin√©, extraction de {filename}...")
            
            # Extraire le fichier voulu
            zip_buffer.seek(0)
            with zipfile.ZipFile(zip_buffer, 'r') as zf:
                if filename not in zf.namelist():
                    print(f"‚ùå {filename} non trouv√© dans le ZIP")
                    return None
                    
                with zf.open(filename) as f:
                    data = json.loads(f.read().decode('utf-8'))
            
            # **MODE STREAMING: Extraire seulement les 6 indicateurs cl√©s** (r√©duit 99% de la taille)
            if STREAMING_MODE:
                original_size = len(json.dumps(data)) / (1024 * 1024)
                data = extract_key_metrics_only(data)
                filtered_size = len(json.dumps(data)) / (1024 * 1024)
                print(f"   üóúÔ∏è  Mode Streaming: {original_size:.1f} MB ‚Üí {filtered_size:.1f} MB ({100 * (1 - filtered_size/original_size):.0f}% de r√©duction)")
            # **FILTRAGE: Ne garder que les donn√©es financi√®res** (r√©duit ~90% de la taille)
            elif FINANCIAL_ONLY_MODE:
                original_size = len(json.dumps(data)) / (1024 * 1024)
                data = filter_financial_data_only(data)
                filtered_size = len(json.dumps(data)) / (1024 * 1024)
                print(f"   üóúÔ∏è  Filtrage: {original_size:.1f} MB ‚Üí {filtered_size:.1f} MB ({100 * (1 - filtered_size/original_size):.0f}% de r√©duction)")
            
            # Sauvegarder dans le cache
            CACHE_DIR.mkdir(parents=True, exist_ok=True)
            try:
                with open(cache_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f)
                print(f"üíæ Cache sauvegard√©: {filename}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Impossible de sauvegarder le cache: {e}")
            
            return data
            
        except Exception as e:
            print(f"‚ùå Erreur t√©l√©chargement {filename} (tentative {attempt + 1}): {e}")
            if attempt < FTP_MAX_RETRIES - 1:
                print(f"   üîÑ Nouvelle tentative dans 3 secondes...")
                import time
                time.sleep(3)
            else:
                print(f"‚ùå √âchec d√©finitif apr√®s {FTP_MAX_RETRIES} tentatives")
                return None
    
    return None


def extract_financial_data(bilan: Dict) -> Dict[str, Any]:
    """Extraire les donn√©es financi√®res d'un bilan
    
    Supporte 2 formats :
    1. Format complet (ancien) : avec bilanSaisi.bilan.detail.pages[]
    2. Format streaming (nouveau) : avec metrics directement
    """
    financial_data = {}
    
    # D√©tecter le format
    if "metrics" in bilan:
        # FORMAT STREAMING (nouveau) : extraction ultra-rapide
        metrics_data = bilan.get("metrics", {})
        
        financial_data["date_cloture"] = bilan.get("dateCloture", "")
        financial_data["date_depot"] = bilan.get("dateDepot", "")
        financial_data["denomination"] = ""  # Pas disponible en mode streaming (d√©j√† dans Pappers)
        
        # Convertir les m√©triques
        metrics = {}
        for code, values in metrics_data.items():
            try:
                value_n = values.get("m1", "") if isinstance(values, dict) else ""
                if value_n and value_n.strip():
                    numeric = int(value_n)
                    if numeric > 1000000000:  # En centimes
                        numeric = numeric // 100
                    metrics[code] = numeric
            except:
                pass
        
        financial_data["chiffre_affaires"] = metrics.get("FA")
        financial_data["resultat_net"] = metrics.get("HN")
        financial_data["resultat_exploitation"] = metrics.get("GC")
        financial_data["total_actif"] = metrics.get("BJ")
        financial_data["capitaux_propres"] = metrics.get("DL")
        financial_data["effectif"] = metrics.get("HY")
    
    else:
        # FORMAT COMPLET (ancien) : parsing complet de la structure
        identite = bilan.get("bilanSaisi", {}).get("bilan", {}).get("identite", {})
        financial_data["date_cloture"] = identite.get("dateClotureExercice", "")
        financial_data["date_depot"] = bilan.get("dateDepot", "")
        financial_data["denomination"] = bilan.get("denomination", "")
        
        pages = bilan.get("bilanSaisi", {}).get("bilan", {}).get("detail", {}).get("pages", [])
        
        # Extraire les liasses
        metrics = {}
        for page in pages:
            for liasse in page.get("liasses", []):
                code = liasse.get("code", "")
                if code in LIASSE_CODES:
                    try:
                        value_n = liasse.get("m1", "")
                        if value_n and value_n.strip():
                            numeric = int(value_n)
                            if numeric > 1000000000:  # En centimes
                                numeric = numeric // 100
                            metrics[code] = numeric
                    except:
                        pass
        
        financial_data["chiffre_affaires"] = metrics.get("FA")
        financial_data["resultat_net"] = metrics.get("HN")
        financial_data["resultat_exploitation"] = metrics.get("GC")
        financial_data["total_actif"] = metrics.get("BJ")
        financial_data["capitaux_propres"] = metrics.get("DL")
        financial_data["effectif"] = metrics.get("HY")
    
    return financial_data


def enrich_from_api_dinum_and_rne(siren: str, max_bilans: int = 10) -> Dict[str, Any]:
    """
    APPROCHE HYBRIDE:
    1. API DINUM pour les infos de base (gratuit, rapide)
    2. Index RNE pour trouver le fichier (~50 KB)
    3. FTP pour t√©l√©charger seulement le fichier n√©cessaire
    """
    print(f"\nüîç Enrichissement hybride pour SIREN: {siren}")
    
    # 1. Charger l'index
    index_data = load_ranges_index()
    if not index_data:
        return {"success": False, "error": "Index non disponible"}
    
    ranges = index_data['ranges']
    
    # 2. Trouver le fichier
    filename = find_file_for_siren(siren, ranges)
    if not filename:
        return {
            "success": False,
            "error": f"SIREN {siren} hors limites RNE",
            "siren": siren
        }
    
    print(f"üìç Fichier identifi√©: {filename}")
    
    # 3. T√©l√©charger le fichier
    data = download_json_from_ftp(filename, use_cache=True)
    if not data:
        return {
            "success": False,
            "error": "Erreur t√©l√©chargement FTP",
            "siren": siren
        }
    
    # 4. Filtrer par SIREN
    siren = str(siren).zfill(9)
    bilans = [b for b in data if b.get('siren') == siren]
    
    if not bilans:
        return {
            "success": False,
            "error": f"Aucun bilan trouv√© pour {siren}",
            "siren": siren
        }
    
    # 5. Trier et extraire
    bilans.sort(key=lambda x: x.get("dateCloture", ""), reverse=True)
    bilans = bilans[:max_bilans]
    
    financial_history = [extract_financial_data(b) for b in bilans]
    
    print(f"‚úÖ {len(financial_history)} bilan(s) trouv√©(s)")
    
    return {
        "success": True,
        "siren": siren,
        "denomination": bilans[0].get("denomination", ""),
        "nb_bilans": len(financial_history),
        "bilans": financial_history,
        "source": "RNE via FTP INPI"
    }


def enrich_from_rne_only(siren_or_siret: str, max_bilans: int = 10) -> Dict[str, Any]:
    """
    Enrichissement RNE SEUL (sans passer par Pappers).
    
    Utilise uniquement les donn√©es RNE pour r√©cup√©rer :
    - Les informations de base (d√©nomination depuis le bilan)
    - Les donn√©es financi√®res historiques
    
    Id√©al pour les utilisateurs qui ont d√©j√† une liste de SIRETs valid√©s
    et veulent uniquement les donn√©es financi√®res RNE.
    
    Args:
        siren_or_siret: SIREN (9 chiffres) ou SIRET (14 chiffres)
        max_bilans: Nombre maximum de bilans √† r√©cup√©rer
        
    Returns:
        Dict avec success, siren, denomination, nb_bilans, bilans, source
    """
    # Extraire le SIREN du SIRET si n√©cessaire
    siren = siren_or_siret[:9] if len(siren_or_siret) >= 9 else siren_or_siret
    siren = str(siren).zfill(9)
    
    print(f"\nüîç Enrichissement RNE seul pour SIREN: {siren}")
    
    # 1. Charger l'index
    index_data = load_ranges_index()
    if not index_data:
        return {
            "success": False,
            "error": "Index RNE non disponible",
            "siren": siren
        }
    
    ranges = index_data['ranges']
    
    # 2. Trouver le fichier
    filename = find_file_for_siren(siren, ranges)
    if not filename:
        return {
            "success": False,
            "error": f"SIREN {siren} hors limites RNE (pas de donn√©es)",
            "siren": siren
        }
    
    print(f"üìç Fichier RNE identifi√©: {filename}")
    
    # 3. T√©l√©charger/charger le fichier
    data = download_json_from_ftp(filename, use_cache=True)
    if not data:
        return {
            "success": False,
            "error": "Erreur t√©l√©chargement FTP ou lecture cache",
            "siren": siren
        }
    
    # 4. Filtrer par SIREN
    bilans = [b for b in data if b.get('siren') == siren]
    
    if not bilans:
        return {
            "success": False,
            "error": f"Aucun bilan trouv√© pour SIREN {siren} dans RNE",
            "siren": siren
        }
    
    # 5. Trier et extraire les donn√©es financi√®res
    bilans.sort(key=lambda x: x.get("dateCloture", ""), reverse=True)
    bilans = bilans[:max_bilans]
    
    financial_history = [extract_financial_data(b) for b in bilans]
    
    # 6. R√©cup√©rer la d√©nomination depuis le premier bilan
    denomination = bilans[0].get("denomination", f"Entreprise {siren}")
    
    print(f"‚úÖ RNE: {len(financial_history)} bilan(s) trouv√©(s) pour {denomination}")
    
    return {
        "success": True,
        "siren": siren,
        "siret": siren_or_siret if len(siren_or_siret) == 14 else None,
        "denomination": denomination,
        "nb_bilans": len(financial_history),
        "bilans": financial_history,
        "source": "RNE uniquement (sans Pappers)"
    }


def format_amount(amount: Optional[int]) -> str:
    """Formater un montant"""
    if amount is None:
        return "N/A"
    return f"{amount:,}".replace(",", " ") + " ‚Ç¨"


def display_financial_data(data: Dict):
    """Afficher les donn√©es financi√®res"""
    if not data.get("success"):
        print(f"\n‚ùå {data.get('error', 'Erreur')}")
        return
    
    print(f"\n{'='*80}")
    print(f"üìä {data['denomination']}")
    print(f"    SIREN: {data['siren']} | Source: {data.get('source', 'RNE')}")
    print('='*80)
    
    for bilan in data['bilans'][:5]:  # 5 derniers exercices
        print(f"\nüìÖ {bilan['date_cloture']}")
        print(f"   üí∞ CA: {format_amount(bilan['chiffre_affaires'])}")
        print(f"   üìà R√©sultat net: {format_amount(bilan['resultat_net'])}")
        
        if bilan['effectif']:
            print(f"   üë• Effectif: {bilan['effectif']} personnes")
    
    print(f"\n{'='*80}")


def group_sirens_by_rne_file(sirens: List[str]) -> Dict[str, List[str]]:
    """
    Grouper les SIRENs par fichier RNE pour traitement par lots.
    Retourne: {"filename.json": [siren1, siren2, ...], ...}
    """
    index_data = load_ranges_index()
    if not index_data:
        return {}
    
    ranges = index_data['ranges']
    grouped = {}
    not_found = []
    
    for siren in sirens:
        filename = find_file_for_siren(siren, ranges)
        if filename:
            if filename not in grouped:
                grouped[filename] = []
            grouped[filename].append(siren)
        else:
            not_found.append(siren)
    
    if not_found:
        print(f"‚ö†Ô∏è  {len(not_found)} SIREN(s) hors limites RNE: {not_found[:5]}{'...' if len(not_found) > 5 else ''}")
    
    return grouped


def process_batch(filename: str, sirens: List[str], max_bilans: int = 10, cleanup: bool = True) -> Dict[str, Dict]:
    """
    Traiter un lot de SIRENs depuis un m√™me fichier RNE.
    
    Args:
        filename: Nom du fichier RNE
        sirens: Liste des SIRENs √† extraire
        max_bilans: Nombre max de bilans par SIREN
       cleanup: Supprimer le fichier du cache apr√®s traitement
    
    Returns:
        {siren: {donn√©es enrichies}, ...}
    """
    print(f"\nüì¶ Traitement du lot: {filename} ({len(sirens)} entreprise(s))")
    
    # En mode espace limit√©, toujours nettoyer sauf si d√©j√† en cache (< 000085)
    if LIMITED_SPACE_MODE:
        file_num = int(filename.replace('stock_', '').replace('.json', ''))
        cleanup = cleanup and file_num >= 85  # Garder les 84 premiers fichiers
    
    # 1. T√©l√©charger le fichier RNE
    data = download_json_from_ftp(filename, use_cache=True)
    if not data:
        return {siren: {"success": False, "error": "Erreur t√©l√©chargement"} for siren in sirens}
    
    results = {}
    
    # 2. Traiter chaque SIREN
    for siren in sirens:
        siren_padded = str(siren).zfill(9)
        bilans = [b for b in data if b.get('siren') == siren_padded]
        
        if bilans:
            bilans.sort(key=lambda x: x.get("dateCloture", ""), reverse=True)
            bilans = bilans[:max_bilans]
            financial_history = [extract_financial_data(b) for b in bilans]
            
            results[siren] = {
                "success": True,
                "siren": siren_padded,
                "denomination": bilans[0].get("denomination", ""),
                "nb_bilans": len(financial_history),
                "bilans": financial_history,
                "source": "RNE via FTP INPI"
            }
        else:
            results[siren] = {
                "success": False,
                "error": f"Aucun bilan pour {siren}",
                "siren": siren_padded
            }
    
    # 3. Nettoyage du cache si demand√©
    if cleanup:
        cache_path = CACHE_DIR / filename
        if cache_path.exists():
            try:
                cache_path.unlink()
                print(f"üóëÔ∏è  Cache nettoy√©: {filename}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur nettoyage cache {filename}: {e}")
    
    print(f"‚úÖ Lot termin√©: {filename} ({len([r for r in results.values() if r.get('success')])} r√©ussis)")
    return results


def enrich_batch_parallel(sirens: List[str], max_bilans: int = 10, max_workers: int = MAX_CONCURRENT_FILES, 
                          progress_callback=None) -> Dict[str, Dict]:
    """
    Enrichir un lot de SIRENs en parall√®le avec gestion optimis√©e de la m√©moire.
    
    Args:
        sirens: Liste de SIRENs √† enrichir
        max_bilans: Nombre max de bilans par SIREN
        max_workers: Nombre max de fichiers RNE t√©l√©charg√©s simultan√©ment
        progress_callback: Fonction appel√©e avec (completed, total, current_file)
    
    Returns:
        {siren: {donn√©es enrichies}, ...}
    """
    print(f"\n{'='*80}")
    print(f"üöÄ ENRICHISSEMENT PAR LOTS OPTIMIS√â")
    print(f"{'='*80}")
    print(f"üìä Total: {len(sirens)} entreprise(s)")
    print(f"‚öôÔ∏è  Workers: {max_workers} fichiers RNE simultan√©s max")
    print(f"{'='*80}\n")
    
    # 1. Grouper par fichier RNE
    grouped = group_sirens_by_rne_file(sirens)
    total_files = len(grouped)
    
    if not grouped:
        print("‚ùå Aucun SIREN valide √† traiter")
        return {}
    
    print(f"üìÅ R√©partition: {total_files} fichier(s) RNE √† traiter")
    for filename, file_sirens in list(grouped.items())[:5]:
        print(f"   ‚Ä¢ {filename}: {len(file_sirens)} entreprise(s)")
    if total_files > 5:
        print(f"   ... et {total_files - 5} autres fichiers\n")
    
    # 2. Traitement parall√®le par lots
    all_results = {}
    completed_files = 0
    lock = threading.Lock()
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Soumettre tous les lots
        future_to_file = {
            executor.submit(process_batch, filename, file_sirens, max_bilans, cleanup=True): filename
            for filename, file_sirens in grouped.items()
        }
        
        # R√©cup√©rer les r√©sultats au fur et √† mesure
        for future in as_completed(future_to_file):
            filename = future_to_file[future]
            try:
                batch_results = future.result()
                with lock:
                    all_results.update(batch_results)
                    completed_files += 1
                    
                if progress_callback:
                    progress_callback(completed_files, total_files, filename)
                    
            except Exception as e:
                print(f"‚ùå Erreur traitement {filename}: {e}")
                with lock:
                    completed_files += 1
    
    # 3. Statistiques finales
    successful = len([r for r in all_results.values() if r.get('success')])
    print(f"\n{'='*80}")
    print(f"üìä R√âSULTATS FINAUX")
    print(f"{'='*80}")
    print(f"‚úÖ R√©ussis: {successful}/{len(sirens)}")
    print(f"‚ùå √âchecs: {len(sirens) - successful}/{len(sirens)}")
    print(f"üìÅ Fichiers trait√©s: {completed_files}/{total_files}")
    print(f"{'='*80}\n")
    
    return all_results


# Test
if __name__ == "__main__":
    print("="*80)
    print("üèõÔ∏è  MODULE RNE - APPROCHE HYBRIDE OPTIMIS√âE")
    print("="*80)
    print("\nüí° API DINUM + Index ultra-l√©ger (50 KB) + FTP √† la demande")
    print()
    
    # Test simple
    print("\n=== TEST SIMPLE ===")
    test_siren = "552100554"  # EDF
    result = enrich_from_api_dinum_and_rne(test_siren, max_bilans=5)
    display_financial_data(result)
    
    # Test par lots
    print("\n\n=== TEST PAR LOTS ===")
    test_sirens = ["552100554", "005880596", "775665019"]  # EDF + 2 autres
    batch_results = enrich_batch_parallel(test_sirens, max_bilans=3, max_workers=2)
    
    print("\nüìã R√©sultats:")
    for siren, data in batch_results.items():
        if data.get('success'):
            print(f"‚úÖ {siren}: {data.get('nb_bilans', 0)} bilan(s) - {data.get('denomination', 'N/A')}")
        else:
            print(f"‚ùå {siren}: {data.get('error', 'Erreur inconnue')}")
